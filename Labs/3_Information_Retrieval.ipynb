{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "3_Information_Retrieval.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3QKP_UCT2ZY"
      },
      "source": [
        "## Regular Expressions\n",
        "\n",
        "Defining REs in Python is straightforward:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvtuSQ_KT2Za"
      },
      "source": [
        "import re\n",
        "\n",
        "pattern = re.compile('[bcrh]at')\n",
        "pattern2 = re.compile('(.*)([bcrh]at)(.*)') #3 groups : each group between ()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aUlppMLT2Ze"
      },
      "source": [
        "We can then use the pattern to `search()` or `match()` strings to it. \n",
        "\n",
        "`search()` will return a result if the pattern occurs **anywhere** in the input string.\n",
        "\n",
        "`match()` will only return a result if the pattern **completely** matches the input string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-SDdrJQT2Zf"
      },
      "source": [
        "word = 'the batter won the game'\n",
        "matches = re.match(pattern2, word) # won't return a a result, i.e., matches = None\n",
        "searches = re.search(pattern, word) # finds a substring"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOCJXT-g95be",
        "outputId": "a261bfee-8c41-4412-b397-ba1477a0fa93"
      },
      "source": [
        "print(matches)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<re.Match object; span=(0, 23), match='the batter won the game'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WA0p84TUT2Zh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97bc6543-1ca8-4a1c-c920-c4e747891def"
      },
      "source": [
        "print(matches.groups()) #baranthesis split groups \n",
        "print(searches) #starts at 4 and ends at 7"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('the ', 'bat', 'ter won the game')\n",
            "<re.Match object; span=(4, 7), match='bat'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSrth_iAT2Zl"
      },
      "source": [
        "Both have a number of attributes to access the results. \n",
        "- `span()` gives us a tuple of the substring that matches\n",
        "- `group()`returns the matched substring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ddb7SJqoT2Zl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de7a20ce-76cf-4814-d3f7-a87145a27028"
      },
      "source": [
        "span = searches.span() #span[0] index of the first char, span[1] index of the last char\n",
        "word[span[0]:span[1]], span"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('bat', (4, 7))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rn3u2KVJT2Zn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "42cc37fd-6222-44d1-bb39-55465aeaf88c"
      },
      "source": [
        "searches.group()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'bat'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1U0OciCT2Zp"
      },
      "source": [
        "If we have used several RE groups (in brackets `()`), we can access them individually via `groups()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqav7Tb9T2Zt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e33674d9-5058-49a9-ea2e-11ccc5349fd2"
      },
      "source": [
        "word = 'preconstitutionalism'\n",
        "affixes = re.compile('(...).+(...)')\n",
        "re.search(affixes, word).groups()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('pre', 'ism')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ifp6DaKT2Zw"
      },
      "source": [
        "For the email address finder, we can use a more advanced pattern and test it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlECD_hNT2Zw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06c08c8c-bfe3-463b-f151-198097c7e284"
      },
      "source": [
        "email = re.compile('^[A-Za-z0-9][A-Za-z0-9\\.-]*@[A-Za-z0-9][A-Za-z0-9\\.-]+\\.[A-Za-z0-9\\.-][A-Za-z0-9\\.-][A-Za-z0-9\\.-]?$')\n",
        "# for address in ['me.@unibocconi.it', '@web.de', '.@gmx.com', 'not working@aol.com']:\n",
        "\n",
        "for address in 'notMyFault@webmail.com,smithie123@gmx,Free stuff@unibocconi.it,mark_my_words@hotmail;com,truthOrDare@webmail.in,look@me@twitter.com,how2GetAnts@aol.dfdsfgfdsgfd'.split(','):\n",
        "    print(address, re.match(email, address))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "notMyFault@webmail.com <re.Match object; span=(0, 22), match='notMyFault@webmail.com'>\n",
            "smithie123@gmx None\n",
            "Free stuff@unibocconi.it None\n",
            "mark_my_words@hotmail;com None\n",
            "truthOrDare@webmail.in <re.Match object; span=(0, 22), match='truthOrDare@webmail.in'>\n",
            "look@me@twitter.com None\n",
            "how2GetAnts@aol.dfdsfgfdsgfd None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDGTQciGT2Zy"
      },
      "source": [
        "We can also use the pattern to replace elements of a string that match with `sub()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLhT-KCDT2Zz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "681a8c6e-6d02-4f2a-d6fb-41c0342995c7"
      },
      "source": [
        "print('Are you all awake?'.replace('???', '!'))\n",
        "\n",
        "numbers = re.compile('[0-9]')\n",
        "re.sub(numbers, '0', 'Back in the 90s, when I was a 12-year-old, a CD cost just 15,99EUR!')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Are you all awake?\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Back in the 00s, when I was a 00-year-old, a CD cost just 00,00EUR!'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70OwFvZlT2Z2"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "Write a RegEx to remove all user names from the tweets and replace them with the token \"@USER\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9bJ0_itWjnW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1aa812cf-7c2f-435a-d201-afad62e3309f"
      },
      "source": [
        "! pip install wget"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp37-none-any.whl size=9681 sha256=69fc9c581e619e5818bf81f4447e5529b89734711053c284d3f0dfb0a984c50d\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIdcBkkDWjIV"
      },
      "source": [
        "import wget\n",
        "url = 'https://raw.githubusercontent.com/dirkhovy/NLPclass/master/data/tweets_en.txt'\n",
        "wget.download(url, 'tweets_en.txt')\n",
        "tweets = [line.strip() for line in open('tweets_en.txt', encoding='utf8')]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fo5u3VwuBV79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ee901b7-51d2-4a81-d372-0ac61069260b"
      },
      "source": [
        "# your code here\r\n",
        "user_names_pattern = re.compile('@[A-Za-z0-9\\.-_]+')\r\n",
        "user_tweets = [re.sub(user_names_pattern, '@USER', tweet) for tweet in tweets[:20]]\r\n",
        "user_tweets"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@USER I think a lot of people just enjoy being a pain in the ass on there',\n",
              " 'Best get ready sunbed and dinner with nana today :)',\n",
              " '@USER thats awesome!',\n",
              " 'Loving this weather',\n",
              " '‚Äú@USER Just seen an absolute idiot in shorts! Be serious!‚Äù Desperado gentleman',\n",
              " '@USER trying to resist a hardcore rave haha! Resisting towns a doddle! Posh dance floor should wear them in quite easy xx',\n",
              " '59 days until @USER!!! Wooo @USER #cannotwait',\n",
              " 'That was the dumbest tweet i ever seen',\n",
              " 'Oh what to do on this fine sunny day?',\n",
              " '@USER hows the fish ? Hope they r ok. Xx',\n",
              " '@USER üò†',\n",
              " 'Or this @USER http://t.co/Gsb7V1oVLU',\n",
              " '@USER your diary is undoubtedly busier than mine, but feel free to check http://t.co/0pjNL1uwU9',\n",
              " 'Willy',\n",
              " '@USER congrats gorgeous xxx',\n",
              " 'Puppies are hard work!!! So rewarding though, I love my little Bovril so much! http://t.co/a8cHDbGUKo',\n",
              " 'Hungover banter lol hehe what http://t.co/eqbAFlyHng',\n",
              " '@USER Then come down to London and see him. Lol :-) xxxx',\n",
              " \"i'm not going to wear makeup today #whocares #nobody\",\n",
              " '@USER whit']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hueOZuI9WqNk"
      },
      "source": [
        "Now, write a RegEx to extract all user names from the tweets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njHhaKfcWxbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a7631cf-ad81-478e-dd33-627c03547039"
      },
      "source": [
        "# your code here\n",
        "\n",
        "#users_matches = [re.match(user_names_pattern, tweet) for tweet in tweets]\n",
        "#users_matches\n",
        "list_mentions = []\n",
        "for tweet in tweets[:20]:\n",
        "  if re.search(user_names_pattern,tweet) is not None:\n",
        "    list_mentions.append(re.search(user_names_pattern,tweet).group())\n",
        "list_mentions"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@cosmetic_candy',\n",
              " '@hardlyin70',\n",
              " '@danny_boy_37:',\n",
              " '@SamanthaOrmerod',\n",
              " '@Beyonce',\n",
              " '@Brooke_C_X',\n",
              " '@Jbowe_',\n",
              " '@louise_munchi',\n",
              " '@guy_clifton',\n",
              " '@StephanieLee__',\n",
              " '@peterbaird5',\n",
              " '@GreigSweeney']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2_jEmDaHqAz",
        "outputId": "435c19b7-715d-4b10-cf56-70a4dec23b64"
      },
      "source": [
        "tweet = \" Hi @azza\"\r\n",
        "re.findall(user_names_pattern,tweet)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@azza']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txOet6tET2Z6"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "Write a RegEx to search for all hashtags containing the word `good` in them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbECIdiXT2Z6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5e6620a-57af-47c4-f2e1-f3fc66d0d03d"
      },
      "source": [
        "# your code here \r\n",
        "\r\n",
        "list_hash = []\r\n",
        "good_hashtag = re.compile('#[\\w]*[Gg][Oo][Oo][Dd][\\w]*')\r\n",
        "\r\n",
        "for tweet in tweets:\r\n",
        "  found = re.findall(good_hashtag,tweet)\r\n",
        "  if found:\r\n",
        "    print(found)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['#goodone']\n",
            "['#goodtime']\n",
            "['#homeforgood']\n",
            "['#GoodbyeMrchips']\n",
            "['#youresogoodforme']\n",
            "['#goodone']\n",
            "['#goodolddays']\n",
            "['#goodtimes']\n",
            "['#notagoodday']\n",
            "['#skiingisGOOD']\n",
            "['#goodbody']\n",
            "['#naegood']\n",
            "['#goodbyesavings']\n",
            "['#goodtimes']\n",
            "['#toogoodtoyou']\n",
            "['#suchagoodfriend']\n",
            "['#gooddriver']\n",
            "['#needtobeagoodstudent']\n",
            "['#goodnight']\n",
            "['#GoodMood']\n",
            "['#lookssoooogood']\n",
            "['#shesnotthatgoodlooking']\n",
            "['#verygood']\n",
            "['#GoodLuck']\n",
            "['#notgood']\n",
            "['#GoodDay']\n",
            "['#wasteofaperfectlygoodtweet']\n",
            "['#goodyeh']\n",
            "['#goodboy']\n",
            "['#MeetAndGreetForGoodbyeTour']\n",
            "['#goodbye']\n",
            "['#goodgirlfriend']\n",
            "['#goodreviews']\n",
            "['#goodfeeling']\n",
            "['#goodtimenolongtime']\n",
            "['#goodtimes']\n",
            "['#goodtimes', '#goodwin']\n",
            "['#goodmemories']\n",
            "['#good']\n",
            "['#whyareallthegoodmengay']\n",
            "['#goodflow']\n",
            "['#feelinggood']\n",
            "['#goodfriends']\n",
            "['#goodone']\n",
            "['#goodtimes']\n",
            "['#goodpal']\n",
            "['#goodnightssleepneeded']\n",
            "['#SoGood']\n",
            "['#damnitgood']\n",
            "['#notgoodenough']\n",
            "['#itsjustsogood']\n",
            "['#lifeisgood']\n",
            "['#notgood']\n",
            "['#goodluck']\n",
            "['#NotGood']\n",
            "['#GoodTimes']\n",
            "['#goodluck']\n",
            "['#3goodthings']\n",
            "['#goodluck']\n",
            "['#notgoodenough']\n",
            "['#goodnight']\n",
            "['#GoodNight']\n",
            "['#goodread']\n",
            "['#homeforgood']\n",
            "['#LifeIsGood']\n",
            "['#goodolddays']\n",
            "['#gooddayplease']\n",
            "['#GoodGirls']\n",
            "['#sogood']\n",
            "['#goodtimes']\n",
            "['#goodtimes']\n",
            "['#jollygoodbeaujolais']\n",
            "['#goodloanout']\n",
            "['#GOODASSSON']\n",
            "['#good']\n",
            "['#commongoods']\n",
            "['#instagood']\n",
            "['#instagood']\n",
            "['#goodhospitaltrip']\n",
            "['#notgood']\n",
            "['#needagoodlaugh']\n",
            "['#Good']\n",
            "['#lifesgood']\n",
            "['#goodfriend']\n",
            "['#allgoodinthehood']\n",
            "['#goodmorning']\n",
            "['#GOODSLEEPINGPATTERN']\n",
            "['#GoodJob']\n",
            "['#goodlucktoyoumate']\n",
            "['#goodtobehome']\n",
            "['#goodluck']\n",
            "['#good']\n",
            "['#GoodStorageSoloutions']\n",
            "['#lifeisgood']\n",
            "['#goodmemories']\n",
            "['#goodboy']\n",
            "['#goodmorning']\n",
            "['#goodlife']\n",
            "['#FeelingGood']\n",
            "['#lifesgood']\n",
            "['#goodluckwiththat']\n",
            "['#notgood']\n",
            "['#good']\n",
            "['#goodthingscometothosewhowait']\n",
            "['#havinagoodday']\n",
            "['#GoodMemories']\n",
            "['#peoplewhoneedagoodhiding']\n",
            "['#good']\n",
            "['#goodluck']\n",
            "['#nothinggood']\n",
            "['#GoodNight']\n",
            "['#gooddayattheoffice']\n",
            "['#goodplace']\n",
            "['#whosagoodboy']\n",
            "['#goodtv']\n",
            "['#goodweekend']\n",
            "['#goodone']\n",
            "['#probationingoodhands']\n",
            "['#weneedtobegood']\n",
            "['#goodtime']\n",
            "['#GoodLuck']\n",
            "['#toogoodforthat']\n",
            "['#instagood']\n",
            "['#onlygoodthingaboutmondays']\n",
            "['#goodriddance']\n",
            "['#goodtimes']\n",
            "['#goingtobegood']\n",
            "['#heardgoodthings']\n",
            "['#feelsgood']\n",
            "['#ooooshGoodSave']\n",
            "['#goodyear']\n",
            "['#goodmermories']\n",
            "['#goodidea']\n",
            "['#Goodnight']\n",
            "['#loveagoodeyebrowwax']\n",
            "['#SOFUCKINGGOOD']\n",
            "['#GoodStuff']\n",
            "['#lookingood']\n",
            "['#goodolddays']\n",
            "['#itsallgoodthoughasimworking4shiftsthisweek']\n",
            "['#feelinggood']\n",
            "['#needagoodgiggle']\n",
            "['#nevergonnabegoodenough', '#neverwillbegoodenough']\n",
            "['#SETGOODTUESDAYS']\n",
            "['#shewasgood']\n",
            "['#todayisagoodday']\n",
            "['#yummysaladsaregood']\n",
            "['#oooohhesagoodboy']\n",
            "['#goodtimes']\n",
            "['#suprisinggood']\n",
            "['#goodlad']\n",
            "['#verygood']\n",
            "['#goodwork']\n",
            "['#GoodPlayer']\n",
            "['#justtoogood']\n",
            "['#neveragoodsign']\n",
            "['#throughthegoodandthebad']\n",
            "['#WhatAGoodGirl']\n",
            "['#goodtimes']\n",
            "['#notagoodlook']\n",
            "['#GOODLORD']\n",
            "['#AGoodWayToDieHard']\n",
            "['#goodsamaritan']\n",
            "['#GoodNight']\n",
            "['#goodfeeling']\n",
            "['#goodluck']\n",
            "['#NotGood']\n",
            "['#MentionAGoodCouple']\n",
            "['#MentionAGoodCouple']\n",
            "['#feelgood']\n",
            "['#outlooknotgood']\n",
            "['#MentionAGoodCouple']\n",
            "['#MentionAGoodCouple']\n",
            "['#MentionAGoodCouple']\n",
            "['#notgood']\n",
            "['#thankgoodnessitsnottoocold']\n",
            "['#HasDefinitelyQuitForGood']\n",
            "['#goodnight']\n",
            "['#goodneighbour']\n",
            "['#goodfriend']\n",
            "['#goodluck']\n",
            "['#notgoodforyourhealth']\n",
            "['#GoodLord']\n",
            "['#notgood']\n",
            "['#youcantrushagoodsancere']\n",
            "['#goodtimes']\n",
            "['#goodluck']\n",
            "['#GoodGirl']\n",
            "['#goodtimes']\n",
            "['#todayisnotagoodday']\n",
            "['#goodgirl']\n",
            "['#pushitrealgood']\n",
            "['#goodworkmyfriend']\n",
            "['#FeelingGood']\n",
            "['#soundslikeagoodplan']\n",
            "['#GoodDoctor']\n",
            "['#naegood']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TvaZX2iT2Z8"
      },
      "source": [
        "## TF-IDF\n",
        "\n",
        "Let's extract the most important words from Moby Dick"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugMRmSObW6SS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4b12dd06-a4d3-4be8-880b-f275a520f1e2"
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/dirkhovy/NLPclass/master/data/moby_dick.txt'\n",
        "wget.download(url, 'moby_dick.txt')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'moby_dick.txt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tP9bo4MlT2Z8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "861b9c66-4104-4e3e-a843-24225892b13c"
      },
      "source": [
        "import pandas as pd\n",
        "documents = [line.strip() for line in open('moby_dick.txt', encoding='utf8')]\n",
        "print(documents[1])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Call me Ishmael .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kF52UbzbT2Z-"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(analyzer='word',\n",
        "                                   min_df=0.001,\n",
        "                                   max_df=0.75,\n",
        "                                   stop_words='english',\n",
        "                                   sublinear_tf=True)\n",
        "\n",
        "X = tfidf_vectorizer.fit_transform(documents)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3BcEHFMT2Z_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c555d6a-26fe-4975-b82f-fdd36d0b8a22"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9768, 1850)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlzSMGTyT2aB"
      },
      "source": [
        "Now, let's get the same information as raw counts:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-eYAlQoT2aB"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(analyzer='word', min_df=0.001, max_df=0.75, stop_words='english')\n",
        "\n",
        "X2 = vectorizer.fit_transform(documents)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndYI5uoGT2aD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "431ef45e-a89e-4fe9-d1af-5f7c592d749e"
      },
      "source": [
        "X.shape, X2.shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((9768, 1850), (9768, 1850))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTa0ukM-T2aG"
      },
      "source": [
        "df = pd.DataFrame(data={'word': vectorizer.get_feature_names(), \n",
        "                        'tf': X2.sum(axis=0).A1, \n",
        "                        'idf': tfidf_vectorizer.idf_,\n",
        "                        'tfidf': X.sum(axis=0).A1\n",
        "                       })"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "DiP41XtMIwHo",
        "outputId": "bcd24f00-0bf4-49e1-8049-feb939530146"
      },
      "source": [
        "df"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>tf</th>\n",
              "      <th>idf</th>\n",
              "      <th>tfidf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>000</td>\n",
              "      <td>20</td>\n",
              "      <td>7.478919</td>\n",
              "      <td>10.561871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>aboard</td>\n",
              "      <td>21</td>\n",
              "      <td>7.191237</td>\n",
              "      <td>9.852074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>absent</td>\n",
              "      <td>10</td>\n",
              "      <td>7.789074</td>\n",
              "      <td>3.251602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>according</td>\n",
              "      <td>26</td>\n",
              "      <td>6.928873</td>\n",
              "      <td>8.937477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>account</td>\n",
              "      <td>32</td>\n",
              "      <td>6.721233</td>\n",
              "      <td>10.686999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1845</th>\n",
              "      <td>yield</td>\n",
              "      <td>15</td>\n",
              "      <td>7.414381</td>\n",
              "      <td>5.941932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1846</th>\n",
              "      <td>yojo</td>\n",
              "      <td>17</td>\n",
              "      <td>7.702063</td>\n",
              "      <td>4.528857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1847</th>\n",
              "      <td>yon</td>\n",
              "      <td>10</td>\n",
              "      <td>7.789074</td>\n",
              "      <td>5.455786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1848</th>\n",
              "      <td>yonder</td>\n",
              "      <td>18</td>\n",
              "      <td>7.296598</td>\n",
              "      <td>9.507531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1849</th>\n",
              "      <td>young</td>\n",
              "      <td>80</td>\n",
              "      <td>5.830261</td>\n",
              "      <td>25.685623</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1850 rows √ó 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           word  tf       idf      tfidf\n",
              "0           000  20  7.478919  10.561871\n",
              "1        aboard  21  7.191237   9.852074\n",
              "2        absent  10  7.789074   3.251602\n",
              "3     according  26  6.928873   8.937477\n",
              "4       account  32  6.721233  10.686999\n",
              "...         ...  ..       ...        ...\n",
              "1845      yield  15  7.414381   5.941932\n",
              "1846       yojo  17  7.702063   4.528857\n",
              "1847        yon  10  7.789074   5.455786\n",
              "1848     yonder  18  7.296598   9.507531\n",
              "1849      young  80  5.830261  25.685623\n",
              "\n",
              "[1850 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPdLl1HlewVd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0adf2f61-ccef-48b6-dd79-5d8e70c4a8c9"
      },
      "source": [
        "X2.sum(axis=0).A1"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([20, 21, 10, ..., 10, 18, 80], dtype=int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Lb7flVdT2aI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "fc38b0eb-5cd3-484d-ac17-991a905210d0"
      },
      "source": [
        "df = df.sort_values(['tfidf', 'tf', 'idf'], ascending=False)\n",
        "df"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>tf</th>\n",
              "      <th>idf</th>\n",
              "      <th>tfidf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1782</th>\n",
              "      <td>whale</td>\n",
              "      <td>1150</td>\n",
              "      <td>3.262357</td>\n",
              "      <td>224.212236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1838</th>\n",
              "      <td>ye</td>\n",
              "      <td>467</td>\n",
              "      <td>4.257380</td>\n",
              "      <td>153.091587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>231</th>\n",
              "      <td>chapter</td>\n",
              "      <td>171</td>\n",
              "      <td>5.039475</td>\n",
              "      <td>148.370596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>972</th>\n",
              "      <td>man</td>\n",
              "      <td>525</td>\n",
              "      <td>3.982412</td>\n",
              "      <td>134.964448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>922</th>\n",
              "      <td>like</td>\n",
              "      <td>639</td>\n",
              "      <td>3.808543</td>\n",
              "      <td>133.426528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>554</th>\n",
              "      <td>fleet</td>\n",
              "      <td>11</td>\n",
              "      <td>7.702063</td>\n",
              "      <td>3.049731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1423</th>\n",
              "      <td>shortly</td>\n",
              "      <td>10</td>\n",
              "      <td>7.789074</td>\n",
              "      <td>3.032615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1735</th>\n",
              "      <td>valiant</td>\n",
              "      <td>10</td>\n",
              "      <td>7.789074</td>\n",
              "      <td>3.017954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1602</th>\n",
              "      <td>surprise</td>\n",
              "      <td>10</td>\n",
              "      <td>7.789074</td>\n",
              "      <td>2.934600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1071</th>\n",
              "      <td>nations</td>\n",
              "      <td>10</td>\n",
              "      <td>7.789074</td>\n",
              "      <td>2.818093</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1850 rows √ó 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          word    tf       idf       tfidf\n",
              "1782     whale  1150  3.262357  224.212236\n",
              "1838        ye   467  4.257380  153.091587\n",
              "231    chapter   171  5.039475  148.370596\n",
              "972        man   525  3.982412  134.964448\n",
              "922       like   639  3.808543  133.426528\n",
              "...        ...   ...       ...         ...\n",
              "554      fleet    11  7.702063    3.049731\n",
              "1423   shortly    10  7.789074    3.032615\n",
              "1735   valiant    10  7.789074    3.017954\n",
              "1602  surprise    10  7.789074    2.934600\n",
              "1071   nations    10  7.789074    2.818093\n",
              "\n",
              "[1850 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvOWdD-CfCwS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "e4959ff8-1aa7-4c87-dcb8-6567e653228b"
      },
      "source": [
        "df = df.sort_values(['tf', 'idf'], ascending=False)\n",
        "df"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>tf</th>\n",
              "      <th>idf</th>\n",
              "      <th>tfidf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1782</th>\n",
              "      <td>whale</td>\n",
              "      <td>1150</td>\n",
              "      <td>3.262357</td>\n",
              "      <td>224.212236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>922</th>\n",
              "      <td>like</td>\n",
              "      <td>639</td>\n",
              "      <td>3.808543</td>\n",
              "      <td>133.426528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>972</th>\n",
              "      <td>man</td>\n",
              "      <td>525</td>\n",
              "      <td>3.982412</td>\n",
              "      <td>134.964448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>ahab</td>\n",
              "      <td>511</td>\n",
              "      <td>4.019453</td>\n",
              "      <td>131.484086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1414</th>\n",
              "      <td>ship</td>\n",
              "      <td>509</td>\n",
              "      <td>4.006953</td>\n",
              "      <td>111.771529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>407</th>\n",
              "      <td>downward</td>\n",
              "      <td>10</td>\n",
              "      <td>7.789074</td>\n",
              "      <td>3.111894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1423</th>\n",
              "      <td>shortly</td>\n",
              "      <td>10</td>\n",
              "      <td>7.789074</td>\n",
              "      <td>3.032615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1735</th>\n",
              "      <td>valiant</td>\n",
              "      <td>10</td>\n",
              "      <td>7.789074</td>\n",
              "      <td>3.017954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1602</th>\n",
              "      <td>surprise</td>\n",
              "      <td>10</td>\n",
              "      <td>7.789074</td>\n",
              "      <td>2.934600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1071</th>\n",
              "      <td>nations</td>\n",
              "      <td>10</td>\n",
              "      <td>7.789074</td>\n",
              "      <td>2.818093</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1850 rows √ó 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          word    tf       idf       tfidf\n",
              "1782     whale  1150  3.262357  224.212236\n",
              "922       like   639  3.808543  133.426528\n",
              "972        man   525  3.982412  134.964448\n",
              "21        ahab   511  4.019453  131.484086\n",
              "1414      ship   509  4.006953  111.771529\n",
              "...        ...   ...       ...         ...\n",
              "407   downward    10  7.789074    3.111894\n",
              "1423   shortly    10  7.789074    3.032615\n",
              "1735   valiant    10  7.789074    3.017954\n",
              "1602  surprise    10  7.789074    2.934600\n",
              "1071   nations    10  7.789074    2.818093\n",
              "\n",
              "[1850 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1JwqX8nT2aK"
      },
      "source": [
        "## Exercise\n",
        "Extract **only** the bigrams (no unigrams) from Moby Dick and find the top 10 in terms of TF-IDF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYjTXRXJT2aK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "6c7b9de2-3d43-4c17-926f-88c728d9fd81"
      },
      "source": [
        "# your code \n",
        "tfidf_vectorizer_bigrams = TfidfVectorizer(analyzer='word',\n",
        "                                   min_df=0.001,\n",
        "                                   max_df=0.75,\n",
        "                                   stop_words='english',\n",
        "                                   sublinear_tf=True,\n",
        "                                   ngram_range = (2,2)\n",
        "                                   )\n",
        "X3 = tfidf_vectorizer_bigrams.fit_transform(documents)\n",
        "vectorizer_bigrams = CountVectorizer(analyzer='word', min_df=0.001, max_df=0.75, stop_words='english', ngram_range=(2,2))\n",
        "X4 = vectorizer_bigrams.fit_transform(documents)\n",
        "\n",
        "df_bigrams = pd.DataFrame(data={'word': vectorizer_bigrams.get_feature_names(), \n",
        "                        'tf': X4.sum(axis=0).A1, \n",
        "                        'idf': tfidf_vectorizer_bigrams.idf_,\n",
        "                        'tfidf': X3.sum(axis=0).A1\n",
        "                       })\n",
        "df_bigrams = df_bigrams.sort_values(['tfidf', 'tf', 'idf'], ascending=False)\n",
        "df_bigrams[:10]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>tf</th>\n",
              "      <th>idf</th>\n",
              "      <th>tfidf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>sperm whale</td>\n",
              "      <td>176</td>\n",
              "      <td>5.051171</td>\n",
              "      <td>143.425771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>white whale</td>\n",
              "      <td>106</td>\n",
              "      <td>5.581799</td>\n",
              "      <td>89.700192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>old man</td>\n",
              "      <td>81</td>\n",
              "      <td>5.780250</td>\n",
              "      <td>73.301252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>moby dick</td>\n",
              "      <td>83</td>\n",
              "      <td>5.817522</td>\n",
              "      <td>68.840652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>captain ahab</td>\n",
              "      <td>64</td>\n",
              "      <td>6.043835</td>\n",
              "      <td>53.911830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>right whale</td>\n",
              "      <td>55</td>\n",
              "      <td>6.161618</td>\n",
              "      <td>46.437100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>mast head</td>\n",
              "      <td>47</td>\n",
              "      <td>6.315768</td>\n",
              "      <td>41.407495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>mast heads</td>\n",
              "      <td>36</td>\n",
              "      <td>6.576051</td>\n",
              "      <td>32.451617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>cried ahab</td>\n",
              "      <td>33</td>\n",
              "      <td>6.660609</td>\n",
              "      <td>31.403016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>whale ship</td>\n",
              "      <td>33</td>\n",
              "      <td>6.660609</td>\n",
              "      <td>29.261082</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            word   tf       idf       tfidf\n",
              "56   sperm whale  176  5.051171  143.425771\n",
              "73   white whale  106  5.581799   89.700192\n",
              "43       old man   81  5.780250   73.301252\n",
              "37     moby dick   83  5.817522   68.840652\n",
              "8   captain ahab   64  6.043835   53.911830\n",
              "48   right whale   55  6.161618   46.437100\n",
              "35     mast head   47  6.315768   41.407495\n",
              "36    mast heads   36  6.576051   32.451617\n",
              "12    cried ahab   33  6.660609   31.403016\n",
              "71    whale ship   33  6.660609   29.261082"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xz1deDO7T2aN"
      },
      "source": [
        "## PMI\n",
        "Extracting PMI from text is relatively straightforward, and `nltk` offer some functions to do so flexibly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDE5YBHZT2aN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af5cfe4d-3eab-435a-f7ba-326d69c5a207"
      },
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "xMeicqRyT2aP"
      },
      "source": [
        "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "stopwords_ = set(stopwords.words('english'))\n",
        "\n",
        "words = [word.lower() for document in documents for word in document.split() \n",
        "         if len(word) > 2 \n",
        "         and word not in stopwords_]\n",
        "         \n",
        "finder = BigramCollocationFinder.from_words(words)\n",
        "bgm = BigramAssocMeasures()\n",
        "score = bgm.mi_like\n",
        "collocations = {'_'.join(bigram): pmi for bigram, pmi in finder.score_ngrams(score)}\n",
        "#collocations"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWtNCk8ojYRd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a3cf9b9-f72f-4363-b8cd-87a9ede31060"
      },
      "source": [
        "finder.score_ngrams(score)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('moby', 'dick'), 83.0),\n",
              " (('sperm', 'whale'), 20.002847184002935),\n",
              " (('mrs', 'hussey'), 10.5625),\n",
              " (('mast', 'heads'), 4.391152941176471),\n",
              " (('sag', 'harbor'), 4.0),\n",
              " (('vinegar', 'cruet'), 4.0),\n",
              " (('try', 'works'), 3.7944046844502277),\n",
              " (('dough', 'boy'), 3.7067873303167422),\n",
              " (('white', 'whale'), 3.698807453416149),\n",
              " (('caw', 'caw'), 3.4722222222222223),\n",
              " (('samuel', 'enderby'), 3.4285714285714284),\n",
              " (('cape', 'horn'), 3.4133333333333336),\n",
              " (('new', 'bedford'), 3.3402061855670104),\n",
              " (('quarter', 'deck'), 3.2339339991315676),\n",
              " (('deacon', 'deuteronomy'), 3.2),\n",
              " (('father', 'mapple'), 3.0),\n",
              " (('gamy', 'jesty'), 3.0),\n",
              " (('hoky', 'poky'), 3.0),\n",
              " (('jesty', 'joky'), 3.0),\n",
              " (('joky', 'hoky'), 3.0),\n",
              " (('sporty', 'gamy'), 3.0),\n",
              " (('sulk', 'pout'), 3.0),\n",
              " (('twos', 'threes'), 3.0),\n",
              " (('mast', 'head'), 2.464640949554896),\n",
              " (('000', 'lbs'), 2.45),\n",
              " (('chief', 'mate'), 2.4075114075114077),\n",
              " (('old', 'man'), 2.269660474055093),\n",
              " (('straits', 'sunda'), 2.25),\n",
              " (('crow', 'nest'), 2.227272727272727),\n",
              " (('crested', 'comb'), 2.0),\n",
              " (('daboll', 'arithmetic'), 2.0),\n",
              " (('distension', 'contraction'), 2.0),\n",
              " (('gemini', 'twins'), 2.0),\n",
              " (('helter', 'skelter'), 2.0),\n",
              " (('hogs', 'bristles'), 2.0),\n",
              " (('kith', 'kin'), 2.0),\n",
              " (('lirra', 'skirra'), 2.0),\n",
              " (('pell', 'mell'), 2.0),\n",
              " (('rio', 'plata'), 2.0),\n",
              " (('ruinous', 'discount'), 2.0),\n",
              " (('sagittarius', 'archer'), 2.0),\n",
              " (('sprinkling', 'mistifying'), 2.0),\n",
              " (('squaw', 'tistig'), 2.0),\n",
              " (('tolland', 'county'), 2.0),\n",
              " (('veriest', 'trifles'), 2.0),\n",
              " (('heidelburgh', 'tun'), 1.9285714285714286),\n",
              " (('seams', 'dents'), 1.8285714285714285),\n",
              " (('aunt', 'charity'), 1.7777777777777777),\n",
              " (('lamp', 'feeder'), 1.6896551724137931),\n",
              " (('latitude', 'longitude'), 1.6025641025641026),\n",
              " (('frederick', 'cuvier'), 1.6),\n",
              " (('don', 'sebastian'), 1.5802469135802468),\n",
              " (('years', 'ago'), 1.5093701996927804),\n",
              " (('ding', 'dong'), 1.5),\n",
              " (('dong', 'ding'), 1.5),\n",
              " (('pre', 'adamite'), 1.5),\n",
              " (('captain', 'ahab'), 1.4964182480834485),\n",
              " (('chapter', 'the'), 1.4210526315789473),\n",
              " (('warp', 'woof'), 1.3736263736263736),\n",
              " (('aye', 'aye'), 1.3639125910509886),\n",
              " (('captain', 'peleg'), 1.3541615009504917),\n",
              " (('550', 'ankers'), 1.3333333333333333),\n",
              " (('cretan', 'labyrinth'), 1.3333333333333333),\n",
              " (('davy', 'jones'), 1.3333333333333333),\n",
              " (('kee', 'hee'), 1.3333333333333333),\n",
              " (('renegades', 'castaways'), 1.3333333333333333),\n",
              " (('similitude', 'ceases'), 1.3333333333333333),\n",
              " (('texel', 'leyden'), 1.3333333333333333),\n",
              " (('iii', 'duodecimo'), 1.3020833333333333),\n",
              " (('funny', 'sporty'), 1.2857142857142858),\n",
              " (('jack', 'knife'), 1.1797235023041475),\n",
              " (('marchant', 'service'), 1.1636363636363636),\n",
              " (('lower', 'jaw'), 1.1428571428571428),\n",
              " (('whosoever', 'raises'), 1.125),\n",
              " (('spouter', 'inn'), 1.1160714285714286),\n",
              " (('pivot', 'hole'), 1.1136363636363635),\n",
              " (('seventy', 'seventh'), 1.0909090909090908),\n",
              " (('absent', 'minded'), 1.0666666666666667),\n",
              " (('seven', 'hundred'), 1.0626822157434401),\n",
              " (('1750', '1788'), 1.0),\n",
              " (('abstained', 'patrolling'), 1.0),\n",
              " (('abstract', 'unfractioned'), 1.0),\n",
              " (('accidental', 'advantages'), 1.0),\n",
              " (('accountants', 'computed'), 1.0),\n",
              " (('achilles', 'shield'), 1.0),\n",
              " (('administered', 'vernacular'), 1.0),\n",
              " (('adoring', 'cherubim'), 1.0),\n",
              " (('afflicted', 'jaundice'), 1.0),\n",
              " (('affluent', 'cultivated'), 1.0),\n",
              " (('agassiz', 'imagines'), 1.0),\n",
              " (('agrarian', 'freebooting'), 1.0),\n",
              " (('aides', 'marshals'), 1.0),\n",
              " (('alb', 'tunic'), 1.0),\n",
              " (('albert', 'durer'), 1.0),\n",
              " (('alexanders', 'parcelling'), 1.0),\n",
              " (('allurings', 'girlish'), 1.0),\n",
              " (('ambergriese', 'paunch'), 1.0),\n",
              " (('amend', 'reports'), 1.0),\n",
              " (('amidst', 'rustiness'), 1.0),\n",
              " (('amounts', 'butchering'), 1.0),\n",
              " (('amphitheatrical', 'heights'), 1.0),\n",
              " (('anacharsis', 'clootz'), 1.0),\n",
              " (('ancestry', 'posterity'), 1.0),\n",
              " (('andrew', 'jackson'), 1.0),\n",
              " (('animate', 'inanimate'), 1.0),\n",
              " (('annawon', 'headmost'), 1.0),\n",
              " (('antemosaic', 'unsourced'), 1.0),\n",
              " (('anti', 'scorbutic'), 1.0),\n",
              " (('anus', 'breasts'), 1.0),\n",
              " (('append', 'initials'), 1.0),\n",
              " (('approve', 'omnisciently'), 1.0),\n",
              " (('approvingly', 'coax'), 1.0),\n",
              " (('argo', 'navis'), 1.0),\n",
              " (('arkansas', 'duellist'), 1.0),\n",
              " (('aroostook', 'hemlock'), 1.0),\n",
              " (('arrantest', 'topers'), 1.0),\n",
              " (('aspersion', 'disproved'), 1.0),\n",
              " (('asphaltic', 'pavement'), 1.0),\n",
              " (('asphaltites', 'unforgiven'), 1.0),\n",
              " (('assails', 'chases'), 1.0),\n",
              " (('atrocious', 'scoundrel'), 1.0),\n",
              " (('australian', 'settlement'), 1.0),\n",
              " (('authorized', 'legislative'), 1.0),\n",
              " (('avers', 'earl'), 1.0),\n",
              " (('baboon', 'vows'), 1.0),\n",
              " (('bakers', 'bankers'), 1.0),\n",
              " (('baleful', 'comets'), 1.0),\n",
              " (('balena', 'vero'), 1.0),\n",
              " (('baliene', 'ordinaire'), 1.0),\n",
              " (('balmy', 'autumnal'), 1.0),\n",
              " (('barest', 'ruggedest'), 1.0),\n",
              " (('barges', 'actium'), 1.0),\n",
              " (('bartholomew', 'diaz'), 1.0),\n",
              " (('bass', 'viol'), 1.0),\n",
              " (('basso', 'relievo'), 1.0),\n",
              " (('bedside', 'squatted'), 1.0),\n",
              " (('befogged', 'bedeadened'), 1.0),\n",
              " (('belated', 'innocently'), 1.0),\n",
              " (('belial', 'bondsman'), 1.0),\n",
              " (('bespattering', 'glorying'), 1.0),\n",
              " (('bess', 'gallantly'), 1.0),\n",
              " (('betty', 'snarles'), 1.0),\n",
              " (('beverage', 'circulates'), 1.0),\n",
              " (('bewildering', 'mediums'), 1.0),\n",
              " (('bigot', 'fadeless'), 1.0),\n",
              " (('blackened', 'elevations'), 1.0),\n",
              " (('boggy', 'soggy'), 1.0),\n",
              " (('bolder', 'waded'), 1.0),\n",
              " (('bombay', 'apollo'), 1.0),\n",
              " (('bombazine', 'cloak'), 1.0),\n",
              " (('bondsman', 'spurn'), 1.0),\n",
              " (('booble', 'alley'), 1.0),\n",
              " (('borean', 'dismasting'), 1.0),\n",
              " (('bountifully', 'laughable'), 1.0),\n",
              " (('bowings', 'scrapings'), 1.0),\n",
              " (('boyhood', 'thoughtless'), 1.0),\n",
              " (('braining', 'feats'), 1.0),\n",
              " (('brats', 'tinkerings'), 1.0),\n",
              " (('braves', 'mustered'), 1.0),\n",
              " (('breasts', 'extend'), 1.0),\n",
              " (('bridegroom', 'clasp'), 1.0),\n",
              " (('brides', 'benignity'), 1.0),\n",
              " (('bridge', 'warningly'), 1.0),\n",
              " (('briefly', 'exhibit'), 1.0),\n",
              " (('brindled', 'cow'), 1.0),\n",
              " (('brisson', 'marten'), 1.0),\n",
              " (('bubbled', 'seethed'), 1.0),\n",
              " (('burghers', 'calais'), 1.0),\n",
              " (('buttered', 'judgmatically'), 1.0),\n",
              " (('butteries', 'cheeseries'), 1.0),\n",
              " (('bystanders', 'zoroaster'), 1.0),\n",
              " (('cabalistics', 'keystone'), 1.0),\n",
              " (('caesarian', 'heir'), 1.0),\n",
              " (('calomel', 'jalap'), 1.0),\n",
              " (('candies', 'maccaroni'), 1.0),\n",
              " (('canny', 'seth'), 1.0),\n",
              " (('caput', 'regina'), 1.0),\n",
              " (('carcass', 'rabid'), 1.0),\n",
              " (('carey', 'chickens'), 1.0),\n",
              " (('cats', 'purr'), 1.0),\n",
              " (('caudam', 'bracton'), 1.0),\n",
              " (('celled', 'honeycombs'), 1.0),\n",
              " (('champions', 'kingly'), 1.0),\n",
              " (('chaotic', 'bundling'), 1.0),\n",
              " (('characterized', 'partial'), 1.0),\n",
              " (('charging', 'twopence'), 1.0),\n",
              " (('cheapest', 'cheeriest'), 1.0),\n",
              " (('chemistry', 'villany'), 1.0),\n",
              " (('childe', 'harold'), 1.0),\n",
              " (('chill', 'lapsed'), 1.0),\n",
              " (('chinks', 'crannies'), 1.0),\n",
              " (('chipping', 'craters'), 1.0),\n",
              " (('chivalric', 'crusaders'), 1.0),\n",
              " (('christenings', 'whom'), 1.0),\n",
              " (('chrysalis', 'roundingly'), 1.0),\n",
              " (('circumventing', 'satirizing'), 1.0),\n",
              " (('civilly', 'moderation'), 1.0),\n",
              " (('clad', 'skins'), 1.0),\n",
              " (('clammy', 'reception'), 1.0),\n",
              " (('claus', 'pott'), 1.0),\n",
              " (('clearer', 'sweetener'), 1.0),\n",
              " (('clootz', 'deputation'), 1.0),\n",
              " (('clothe', 'doubly'), 1.0),\n",
              " (('coalescing', 'gurry'), 1.0),\n",
              " (('cocoa', 'nut'), 1.0),\n",
              " (('cohorts', 'endlessly'), 1.0),\n",
              " (('colds', 'catarrhs'), 1.0),\n",
              " (('collated', 'migrations'), 1.0),\n",
              " (('compactness', 'gloss'), 1.0),\n",
              " (('compunctions', 'suicide'), 1.0),\n",
              " (('conceited', 'ignoramus'), 1.0),\n",
              " (('concussion', 'resounds'), 1.0),\n",
              " (('conflicting', 'aptitudes'), 1.0),\n",
              " (('confound', 'remarking'), 1.0),\n",
              " (('congeal', 'eyelashes'), 1.0),\n",
              " (('conquering', 'earls'), 1.0),\n",
              " (('consecrated', 'consecrating'), 1.0),\n",
              " (('conspicuously', 'label'), 1.0),\n",
              " (('contemplating', 'amputation'), 1.0),\n",
              " (('contented', 'restricting'), 1.0),\n",
              " (('contested', 'election'), 1.0),\n",
              " (('contracts', 'thickens'), 1.0),\n",
              " (('contrastingly', 'concurred'), 1.0),\n",
              " (('contributed', 'receptive'), 1.0),\n",
              " (('controllable', 'occupant'), 1.0),\n",
              " (('convict', 'bunyan'), 1.0),\n",
              " (('copying', 'ducks'), 1.0),\n",
              " (('corinthians', 'corruption'), 1.0),\n",
              " (('corporal', 'animosity'), 1.0),\n",
              " (('countersinkers', 'superiors'), 1.0),\n",
              " (('courting', 'notoriety'), 1.0),\n",
              " (('crafty', 'upraising'), 1.0),\n",
              " (('crappoes', 'frenchmen'), 1.0),\n",
              " (('create', 'unsubduable'), 1.0),\n",
              " (('creative', 'libertines'), 1.0),\n",
              " (('crim', 'con'), 1.0),\n",
              " (('crockett', 'kit'), 1.0),\n",
              " (('cruelty', 'ganders'), 1.0),\n",
              " (('cupola', 'monument'), 1.0),\n",
              " (('czarship', 'withstanding'), 1.0),\n",
              " (('dame', 'isabella'), 1.0),\n",
              " (('damsels', 'caressed'), 1.0),\n",
              " (('daniel', 'boone'), 1.0),\n",
              " (('dauntless', 'stander'), 1.0),\n",
              " (('dauphine', 'paris'), 1.0),\n",
              " (('deadening', 'filliping'), 1.0),\n",
              " (('deathful', 'whaleboat'), 1.0),\n",
              " (('debtor', 'blockhead'), 1.0),\n",
              " (('deceitfully', 'tapered'), 1.0),\n",
              " (('deceiving', 'bedevilling'), 1.0),\n",
              " (('decidedly', 'objectionable'), 1.0),\n",
              " (('decoction', 'seneca'), 1.0),\n",
              " (('defaced', 'unequal'), 1.0),\n",
              " (('del', 'ecuador'), 1.0),\n",
              " (('deluge', 'drowns'), 1.0),\n",
              " (('denominate', 'apparatus'), 1.0),\n",
              " (('denunciations', 'forewarnings'), 1.0),\n",
              " (('deplore', 'inability'), 1.0),\n",
              " (('deriding', 'gesture'), 1.0),\n",
              " (('derision', ';--\"'), 1.0),\n",
              " (('descartian', 'vortices'), 1.0),\n",
              " (('descendants', 'unknowing'), 1.0),\n",
              " (('descriptively', 'treating'), 1.0),\n",
              " (('desolateness', 'reigning'), 1.0),\n",
              " (('detract', 'dramatically'), 1.0),\n",
              " (('deviations', 'azimuth'), 1.0),\n",
              " (('devilishness', 'desperation'), 1.0),\n",
              " (('dexterities', 'sleights'), 1.0),\n",
              " (('diaz', 'reputed'), 1.0),\n",
              " (('dig', 'dig'), 1.0),\n",
              " (('dignified', 'whitewashed'), 1.0),\n",
              " (('diligence', 'leuwenhoeck'), 1.0),\n",
              " (('diluted', 'pickled'), 1.0),\n",
              " (('discharges', 'rifles'), 1.0),\n",
              " (('disciple', 'spurzheim'), 1.0),\n",
              " (('discourse', 'parlors'), 1.0),\n",
              " (('disencumber', 'snarls'), 1.0),\n",
              " (('disgusted', 'carking'), 1.0),\n",
              " (('disjointedly', 'twiske'), 1.0),\n",
              " (('dislike', 'bitterness'), 1.0),\n",
              " (('dismasting', 'blasts'), 1.0),\n",
              " (('dismember', 'dismemberer'), 1.0),\n",
              " (('dissociated', 'characterizing'), 1.0),\n",
              " (('dissolved', 'mayst'), 1.0),\n",
              " (('distilled', 'volatile'), 1.0),\n",
              " (('drawlingly', 'soothingly'), 1.0),\n",
              " (('dreamiest', 'shadiest'), 1.0),\n",
              " (('earl', 'leicester'), 1.0),\n",
              " (('eave', 'troughs'), 1.0),\n",
              " (('eccentric', 'span'), 1.0),\n",
              " (('eddyings', 'angry'), 1.0),\n",
              " (('edmund', 'burke'), 1.0),\n",
              " (('eleventh', 'commandment'), 1.0),\n",
              " (('elks', 'warringly'), 1.0),\n",
              " (('elves', 'heedlessly'), 1.0),\n",
              " (('eminent', 'tremendousness'), 1.0),\n",
              " (('eminently', 'presuming'), 1.0),\n",
              " (('employment', 'frugal'), 1.0),\n",
              " (('emprise', 'weightiest'), 1.0),\n",
              " (('enclosed', 'vitals'), 1.0),\n",
              " (('engrafted', 'clerical'), 1.0),\n",
              " (('enlightened', 'gourmand'), 1.0),\n",
              " (('entitle', 'embarks'), 1.0),\n",
              " (('ephesian', 'sod'), 1.0),\n",
              " (('erromanggoans', 'pannangians'), 1.0),\n",
              " (('esau', 'jacob'), 1.0),\n",
              " (('etherial', 'thrill'), 1.0),\n",
              " (('eventuated', 'liberation'), 1.0),\n",
              " (('evilly', 'protruding'), 1.0),\n",
              " (('evincing', 'observance'), 1.0),\n",
              " (('evoke', 'eyeless'), 1.0),\n",
              " (('exacted', 'implicit'), 1.0),\n",
              " (('excellently', 'spiralizes'), 1.0),\n",
              " (('excluding', 'suburbs'), 1.0),\n",
              " (('executor', 'legatee'), 1.0),\n",
              " (('exegetist', 'supposes'), 1.0),\n",
              " (('exegetists', 'opined'), 1.0),\n",
              " (('exercises', 'boasts'), 1.0),\n",
              " (('expediency', 'conciliating'), 1.0),\n",
              " (('exploding', 'bomb'), 1.0),\n",
              " (('exploring', 'expeditions'), 1.0),\n",
              " (('exported', 'furs'), 1.0),\n",
              " (('fac', 'similes'), 1.0),\n",
              " (('fantasy', 'sipping'), 1.0),\n",
              " (('fasting', 'humiliation'), 1.0),\n",
              " (('fata', 'morgana'), 1.0),\n",
              " (('feasted', 'fatness'), 1.0),\n",
              " (('feastest', 'bloated'), 1.0),\n",
              " (('featured', 'unbodied'), 1.0),\n",
              " (('feebly', 'pointest'), 1.0),\n",
              " (('feegeeans', 'tongatobooarrs'), 1.0),\n",
              " (('feegees', 'tramping'), 1.0),\n",
              " (('feminam', 'mammis'), 1.0),\n",
              " (('fencing', 'boxing'), 1.0),\n",
              " (('ferns', 'grasses'), 1.0),\n",
              " (('festival', 'theology'), 1.0),\n",
              " (('festivities', 'finer'), 1.0),\n",
              " (('fetid', 'closeness'), 1.0),\n",
              " (('filers', 'countersinkers'), 1.0),\n",
              " (('finical', 'criticism'), 1.0),\n",
              " (('fitz', 'swackhammer'), 1.0),\n",
              " (('fixes', 'distortions'), 1.0),\n",
              " (('flinty', 'projections'), 1.0),\n",
              " (('flushed', 'conquest'), 1.0),\n",
              " (('fogo', 'von'), 1.0),\n",
              " (('foie', 'gras'), 1.0),\n",
              " (('forerunning', 'couriers'), 1.0),\n",
              " (('forges', 'melt'), 1.0),\n",
              " (('formally', 'indite'), 1.0),\n",
              " (('forswears', 'disbands'), 1.0),\n",
              " (('fort', 'cattegat'), 1.0),\n",
              " (('fray', 'elemental'), 1.0),\n",
              " (('freewill', 'discriminating'), 1.0),\n",
              " (('frequency', 'recur'), 1.0),\n",
              " (('freshness', 'genuineness'), 1.0),\n",
              " (('frugal', 'housekeepers'), 1.0),\n",
              " (('funereal', 'pyres'), 1.0),\n",
              " (('furred', 'hoar'), 1.0),\n",
              " (('gaffs', 'pikes'), 1.0),\n",
              " (('gallopingly', 'reviewed'), 1.0),\n",
              " (('ganders', 'formally'), 1.0),\n",
              " (('gaseous', 'fata'), 1.0),\n",
              " (('gastric', 'juices'), 1.0),\n",
              " (('gazers', 'circumambulate'), 1.0),\n",
              " (('gazettes', 'extras'), 1.0),\n",
              " (('gem', 'wearer'), 1.0),\n",
              " (('genteel', 'comedies'), 1.0),\n",
              " (('giant', 'holofernes'), 1.0),\n",
              " (('glarings', 'doltish'), 1.0),\n",
              " (('glim', 'jiffy'), 1.0),\n",
              " (('gloomiest', 'reserve'), 1.0),\n",
              " (('gobbles', 'bullets'), 1.0),\n",
              " (('godhead', 'hindoos'), 1.0),\n",
              " (('grains', 'claret'), 1.0),\n",
              " (('grated', 'nutmeg'), 1.0),\n",
              " (('graved', 'ahaz'), 1.0),\n",
              " (('greener', 'fresher'), 1.0),\n",
              " (('greybeards', 'oftenest'), 1.0),\n",
              " (('grocers', 'costermongers'), 1.0),\n",
              " (('growlands', 'walfish'), 1.0),\n",
              " (('gulp', 'shilling'), 1.0),\n",
              " (('habeat', 'caput'), 1.0),\n",
              " (('habergeon', 'esteemeth'), 1.0),\n",
              " (('hacking', 'horrifying'), 1.0),\n",
              " (('hain', 'sittin'), 1.0),\n",
              " (('halfspent', 'suction'), 1.0),\n",
              " (('hallowed', 'precincts'), 1.0),\n",
              " (('ham', 'squattings'), 1.0),\n",
              " (('hangman', 'nooses'), 1.0),\n",
              " (('harvesting', 'hacking'), 1.0),\n",
              " (('hastier', 'withdrawals'), 1.0),\n",
              " (('hastily', 'slewing'), 1.0),\n",
              " (('haughtily', 'courteously'), 1.0),\n",
              " (('heedlessly', 'gambol'), 1.0),\n",
              " (('heir', 'overlording'), 1.0),\n",
              " (('henry', 'viiith'), 1.0),\n",
              " (('hey', 'hey'), 1.0),\n",
              " (('hid', 'heliotrope'), 1.0),\n",
              " (('hillock', 'elbow'), 1.0),\n",
              " (('hollanders', 'zealanders'), 1.0),\n",
              " (('homeless', 'selves'), 1.0),\n",
              " (('hoods', 'ghent'), 1.0),\n",
              " (('horatii', 'pirouetting'), 1.0),\n",
              " (('hotel', 'cluny'), 1.0),\n",
              " (('houseless', 'familyless'), 1.0),\n",
              " (('howls', 'louder'), 1.0),\n",
              " (('hummingly', 'soliloquizes'), 1.0),\n",
              " (('hussar', 'surcoat'), 1.0),\n",
              " (('hydriote', 'canaris'), 1.0),\n",
              " (('hypochondriac', 'supine'), 1.0),\n",
              " (('ibis', 'roasted'), 1.0),\n",
              " (('idolatrous', 'dotings'), 1.0),\n",
              " (('illimitably', 'invaded'), 1.0),\n",
              " (('impalpable', 'destructive'), 1.0),\n",
              " (('imported', 'cobblestones'), 1.0),\n",
              " (('imposed', 'coarse'), 1.0),\n",
              " (('impotent', 'repentant'), 1.0),\n",
              " (('imps', 'blocksburg'), 1.0),\n",
              " (('incited', 'taunts'), 1.0),\n",
              " (('incoherences', 'uninvitedly'), 1.0),\n",
              " (('inconclusive', 'differences'), 1.0),\n",
              " (('incrustations', 'overgrowing'), 1.0),\n",
              " (('indistinctly', 'hesitatingly'), 1.0),\n",
              " (('indite', 'circulars'), 1.0),\n",
              " (('ineffectual', 'guarded'), 1.0),\n",
              " (('inexperience', 'incompetency'), 1.0),\n",
              " (('inferentially', 'negatived'), 1.0),\n",
              " (('infinity', 'firmest'), 1.0),\n",
              " (('infixed', 'unrelenting'), 1.0),\n",
              " (('ingeniously', 'overcome'), 1.0),\n",
              " (('inhospitable', 'wilds'), 1.0),\n",
              " (('inquiries', 'learnt'), 1.0),\n",
              " (('inquisition', 'wanes'), 1.0),\n",
              " (('inscribed', 'unsolved'), 1.0),\n",
              " (('inter', 'indebtedness'), 1.0),\n",
              " (('interferes', 'benevolence'), 1.0),\n",
              " (('interweave', 'antlers'), 1.0),\n",
              " (('intimacy', 'friendliness'), 1.0),\n",
              " (('intrantem', 'feminam'), 1.0),\n",
              " (('inventive', 'unscrupulous'), 1.0),\n",
              " (('inventor', 'patentee'), 1.0),\n",
              " (('inventors', 'patentees'), 1.0),\n",
              " (('invertedly', 'contradict'), 1.0),\n",
              " (('investigated', 'incongruity'), 1.0),\n",
              " (('inwreathing', 'orisons'), 1.0),\n",
              " (('ironical', 'coincidings'), 1.0),\n",
              " (('irresponsible', 'ferociousness'), 1.0),\n",
              " (('isabella', 'inquisition'), 1.0),\n",
              " (('isaiah', 'archangels'), 1.0),\n",
              " (('isthmus', 'darien'), 1.0),\n",
              " (('jackson', 'pebbles'), 1.0),\n",
              " (('jambs', 'bricks'), 1.0),\n",
              " (('japonicas', 'pearls'), 1.0),\n",
              " (('jaundice', 'infirmity'), 1.0),\n",
              " (('jebb', 'anticipative'), 1.0),\n",
              " (('journeyman', 'joiner'), 1.0),\n",
              " (('juan', 'fernandes'), 1.0),\n",
              " (('jungle', 'overlays'), 1.0),\n",
              " (('jure', 'meritoque'), 1.0),\n",
              " (('kentucky', 'mammoth'), 1.0),\n",
              " (('kit', 'carson'), 1.0),\n",
              " (('knights', 'squires'), 1.0),\n",
              " (('knob', 'slamming'), 1.0),\n",
              " (('knobby', 'ostrich'), 1.0),\n",
              " (('knocks', 'coke'), 1.0),\n",
              " (('knotty', 'aroostook'), 1.0),\n",
              " (('koo', 'loo'), 1.0),\n",
              " (('lamatins', 'dugongs'), 1.0),\n",
              " (('lament', 'parents'), 1.0),\n",
              " (('lanes', 'alleys'), 1.0),\n",
              " (('lassitude', 'overtakes'), 1.0),\n",
              " (('lege', 'naturae'), 1.0),\n",
              " (('legislative', 'enactment'), 1.0),\n",
              " (('lens', 'herschel'), 1.0),\n",
              " (('leopards', 'unrecking'), 1.0),\n",
              " (('lessons', 'inculcated'), 1.0),\n",
              " (('leuwenhoeck', 'submits'), 1.0),\n",
              " (('levanter', 'simoon'), 1.0),\n",
              " (('leviathanism', 'signifying'), 1.0),\n",
              " (('lightest', 'corky'), 1.0),\n",
              " (('limestone', 'marl'), 1.0),\n",
              " (('limitless', 'uncharted'), 1.0),\n",
              " (('limpid', 'odoriferous'), 1.0),\n",
              " (('lipless', 'unfeatured'), 1.0),\n",
              " (('listened', 'unhappy'), 1.0),\n",
              " (('liturgies', 'xxxix'), 1.0),\n",
              " (('loitering', 'shady'), 1.0),\n",
              " (('loiters', 'predicted'), 1.0),\n",
              " (('longed', 'vermillion'), 1.0),\n",
              " (('lotus', 'unfolding'), 1.0),\n",
              " (('loungingly', 'managed'), 1.0),\n",
              " (('lovings', 'longings'), 1.0),\n",
              " (('loyal', 'britons'), 1.0),\n",
              " (('lulled', 'opium'), 1.0),\n",
              " (('lunar', 'astral'), 1.0),\n",
              " (('luridly', 'illumined'), 1.0),\n",
              " (('luxuriant', 'profusion'), 1.0),\n",
              " (('maachah', 'judea'), 1.0),\n",
              " (('machine', 'automaton'), 1.0),\n",
              " (('magnify', 'archaeological'), 1.0),\n",
              " (('magniloquent', 'ascriptions'), 1.0),\n",
              " (('maidenly', 'gentleness'), 1.0),\n",
              " (('mammis', 'lactantem'), 1.0),\n",
              " (('manes', 'scowled'), 1.0),\n",
              " (('manfully', 'sheering'), 1.0),\n",
              " (('marius', 'sylla'), 1.0),\n",
              " (('marl', 'bequeathed'), 1.0),\n",
              " (('marsh', 'perpetuates'), 1.0),\n",
              " (('matched', 'overmanned'), 1.0),\n",
              " (('matse', 'avatar'), 1.0),\n",
              " (('matsmai', 'sikoke'), 1.0),\n",
              " (('maury', 'national'), 1.0),\n",
              " (('meanly', 'contemptibly'), 1.0),\n",
              " (('medicament', 'druggists'), 1.0),\n",
              " (('meritoque', 'submitted'), 1.0),\n",
              " (('meshach', 'abednego'), 1.0),\n",
              " (('metempsychosis', 'pythagoras'), 1.0),\n",
              " (('methods', 'intelligently'), 1.0),\n",
              " (('miller', 'shuts'), 1.0),\n",
              " (('minor', 'contingencies'), 1.0),\n",
              " (('mints', 'spanishly'), 1.0),\n",
              " (('miscellaneously', 'carnivorous'), 1.0),\n",
              " (('moaning', 'squadrons'), 1.0),\n",
              " (('mocks', 'dares'), 1.0),\n",
              " (('mohawk', 'counties'), 1.0),\n",
              " (('moidores', 'pistoles'), 1.0),\n",
              " (('momentary', 'impetus'), 1.0),\n",
              " (('monitions', 'unneeded'), 1.0),\n",
              " (('monks', 'dunfermline'), 1.0),\n",
              " (('monsoons', 'pampas'), 1.0),\n",
              " (('moorish', 'scimetars'), 1.0),\n",
              " (('morally', 'enfeebled'), 1.0),\n",
              " (('mountaineers', 'alleghanian'), 1.0),\n",
              " (('movingly', 'admonish'), 1.0),\n",
              " (('mufti', 'thronged'), 1.0),\n",
              " (('multum', 'parvo'), 1.0),\n",
              " (('mummeries', 'unmeaningly'), 1.0),\n",
              " (('murray', 'grammar'), 1.0),\n",
              " (('mutes', 'bowstring'), 1.0),\n",
              " (('nailest', 'geese'), 1.0),\n",
              " (('nat', 'swaine'), 1.0),\n",
              " (('national', 'observatory'), 1.0),\n",
              " (('naturae', 'jure'), 1.0),\n",
              " (('needing', 'supervision'), 1.0),\n",
              " (('neighbor', 'cholo'), 1.0),\n",
              " (('nets', 'mackerel'), 1.0),\n",
              " (('newspaper', 'obituary'), 1.0),\n",
              " (('nibbling', 'goats'), 1.0),\n",
              " (('niphon', 'matsmai'), 1.0),\n",
              " (('nondescript', 'provincialisms'), 1.0),\n",
              " (('nosegays', 'damsels'), 1.0),\n",
              " (('nudging', 'tahitan'), 1.0),\n",
              " (('numbed', 'wasps'), 1.0),\n",
              " (('nun', 'evoke'), 1.0),\n",
              " (('objected', 'reserving'), 1.0),\n",
              " (('observest', 'sashless'), 1.0),\n",
              " (('occult', 'lessons'), 1.0),\n",
              " (('occupant', 'occupants'), 1.0),\n",
              " (('octavoes', '*--'), 1.0),\n",
              " (('odious', 'stigma'), 1.0),\n",
              " (('ointment', 'medicament'), 1.0),\n",
              " (('olmstead', 'rev'), 1.0),\n",
              " (('omnisciently', 'exhaustive'), 1.0),\n",
              " (('oppositely', 'voided'), 1.0),\n",
              " (('orion', 'glitters'), 1.0),\n",
              " (('orleans', 'whiskey'), 1.0),\n",
              " (('ornamental', 'knobs'), 1.0),\n",
              " (('outcries', 'anathemas'), 1.0),\n",
              " (('overburdening', 'panniers'), 1.0),\n",
              " (('overtakes', 'sated'), 1.0),\n",
              " (('pannangians', 'brighggians'), 1.0),\n",
              " (('parisians', 'entrenched'), 1.0),\n",
              " (('partial', 'resemblances'), 1.0),\n",
              " (('partiality', 'undue'), 1.0),\n",
              " (('parvo', 'sheffield'), 1.0),\n",
              " (('pascal', 'rousseau'), 1.0),\n",
              " (('passively', 'await'), 1.0),\n",
              " (('pasteboard', 'masks'), 1.0),\n",
              " (('pate', 'foie'), 1.0),\n",
              " (('patris', 'sed'), 1.0),\n",
              " (('pattern', 'imprimis'), 1.0),\n",
              " (('pealing', 'exultation'), 1.0),\n",
              " (('pedestals', 'statues'), 1.0),\n",
              " (('peltry', 'wigwams'), 1.0),\n",
              " (('penal', 'gout'), 1.0),\n",
              " (('penem', 'intrantem'), 1.0),\n",
              " (('perfumery', 'pastiles'), 1.0),\n",
              " (('philosophically', 'drawled'), 1.0),\n",
              " (('pictorial', 'delusions'), 1.0),\n",
              " (('piers', 'alcoves'), 1.0),\n",
              " (('piggin', 'bailer'), 1.0),\n",
              " (('pilau', 'breadfruit'), 1.0),\n",
              " (('pinned', 'groove'), 1.0),\n",
              " (('piously', 'pounce'), 1.0),\n",
              " (('pippin', 'nick'), 1.0),\n",
              " (('pitted', 'mutilated'), 1.0),\n",
              " (('plaintiveness', 'inwreathing'), 1.0),\n",
              " (('plausible', 'confirmation'), 1.0),\n",
              " (('plums', 'rubies'), 1.0),\n",
              " (('pointless', 'centres'), 1.0),\n",
              " (('poncho', 'slipt'), 1.0),\n",
              " (('postscript', 'behalf'), 1.0),\n",
              " (('pottowottamie', 'sachem'), 1.0),\n",
              " (('powders', 'pomatum'), 1.0),\n",
              " (('praetorians', 'auction'), 1.0),\n",
              " (('precedents', 'utility'), 1.0),\n",
              " (('precedes', 'prophesies'), 1.0),\n",
              " (('preciousness', 'enhancing'), 1.0),\n",
              " (('preparatives', 'needing'), 1.0),\n",
              " (('presaging', 'vibrations'), 1.0),\n",
              " (('proclaimed', 'inheritor'), 1.0),\n",
              " (('proffer', 'passer'), 1.0),\n",
              " (('proverbial', 'evanescence'), 1.0),\n",
              " (('provincial', 'sentimentalist'), 1.0),\n",
              " (('ptolemy', 'philopater'), 1.0),\n",
              " (('purposed', 'befriending'), 1.0),\n",
              " (('quarrel', 'backwoods'), 1.0),\n",
              " (('quenchless', 'feud'), 1.0),\n",
              " (('quietest', 'enchanting'), 1.0),\n",
              " (('ragamuffin', 'rapscallions'), 1.0),\n",
              " (('ramifying', 'heartlessness'), 1.0),\n",
              " (('randolphs', 'hardicanutes'), 1.0),\n",
              " (('rascally', 'asiatics'), 1.0),\n",
              " (('ravished', 'europa'), 1.0),\n",
              " (('raw', 'recruit'), 1.0),\n",
              " (('reddenest', 'palest'), 1.0),\n",
              " (('redolent', 'myrrh'), 1.0),\n",
              " (('reg', 'lar'), 1.0),\n",
              " (('regina', 'caudam'), 1.0),\n",
              " (('regulating', 'circulation'), 1.0),\n",
              " (('remonstrate', 'silenced'), 1.0),\n",
              " (('rending', 'goring'), 1.0),\n",
              " (('renewed', 'onward'), 1.0),\n",
              " (('rensselaers', 'randolphs'), 1.0),\n",
              " (('repentant', 'admonitory'), 1.0),\n",
              " (('replenishes', 'mugs'), 1.0),\n",
              " (('republica', 'del'), 1.0),\n",
              " (('repulses', 'accumulating'), 1.0),\n",
              " (('reputed', 'discoverer'), 1.0),\n",
              " (('residuary', 'legatees'), 1.0),\n",
              " (('restraint', 'tipping'), 1.0),\n",
              " (('rev', 'cheever'), 1.0),\n",
              " (('revelled', 'dalliance'), 1.0),\n",
              " (('rex', 'habeat'), 1.0),\n",
              " (('reydan', 'siskur'), 1.0),\n",
              " (('rigadig', 'tunes'), 1.0),\n",
              " (('rigorous', 'discipline'), 1.0),\n",
              " (('rinaldini', 'insomuch'), 1.0),\n",
              " (('rinaldo', 'rinaldini'), 1.0),\n",
              " (('riotously', 'perverse'), 1.0),\n",
              " (('ripening', 'apricot'), 1.0),\n",
              " (('rivallingly', 'discolour'), 1.0),\n",
              " (('roly', 'poly'), 1.0),\n",
              " (('rondeletius', 'willoughby'), 1.0),\n",
              " (('ross', 'browne'), 1.0),\n",
              " (('routine', 'metempsychosis'), 1.0),\n",
              " (('rue', 'dauphine'), 1.0),\n",
              " (('ruffed', 'mendanna'), 1.0),\n",
              " (('rug', 'softest'), 1.0),\n",
              " (('rumbles', 'talks'), 1.0),\n",
              " (('ruptured', 'membranes'), 1.0),\n",
              " (('rustlings', 'festooned'), 1.0),\n",
              " (('saints', 'demigods'), 1.0),\n",
              " (('same', 'cocked'), 1.0),\n",
              " (('samphire', 'baskets'), 1.0),\n",
              " (('sanctuary', 'wastingly'), 1.0),\n",
              " (('sanctum', 'sanctorum'), 1.0),\n",
              " (('saplings', 'mimicking'), 1.0),\n",
              " (('saul', 'tarsus'), 1.0),\n",
              " (('scheming', 'unappeasedly'), 1.0),\n",
              " (('schmerenburgh', 'smeerenberg'), 1.0),\n",
              " (('scimetars', 'scabbards'), 1.0),\n",
              " (('scolds', 'lesser'), 1.0),\n",
              " (('scornfully', 'champed'), 1.0),\n",
              " (('scorning', 'turnstile'), 1.0),\n",
              " (('scorpio', 'scorpion'), 1.0),\n",
              " (('scorpion', 'stings'), 1.0),\n",
              " (('scrabble', 'scramble'), 1.0),\n",
              " (('scraggy', 'scoria'), 1.0),\n",
              " (('seconds', 'tick'), 1.0),\n",
              " (('seemly', 'correspondence'), 1.0),\n",
              " (('selectest', 'champions'), 1.0),\n",
              " (('seminal', 'germs'), 1.0),\n",
              " (('seneca', 'stoics'), 1.0),\n",
              " (('sequential', 'issues'), 1.0),\n",
              " (('serfs', 'republican'), 1.0),\n",
              " (('serpentine', 'spiralise'), 1.0),\n",
              " (('settlements', 'harems'), 1.0),\n",
              " (('shadiest', 'quietest'), 1.0),\n",
              " (('shadrach', 'meshach'), 1.0),\n",
              " (('shagginess', 'episode'), 1.0),\n",
              " (('shakespeare', 'melancthon'), 1.0),\n",
              " (('shallowest', 'assumption'), 1.0),\n",
              " (('shave', 'sup'), 1.0),\n",
              " (('shields', 'medallions'), 1.0),\n",
              " (('shiftings', 'windrowed'), 1.0),\n",
              " (('shingled', 'attic'), 1.0),\n",
              " (('shrinked', 'sheered'), 1.0),\n",
              " (('shrubs', 'ferns'), 1.0),\n",
              " (('shuts', 'watergate'), 1.0),\n",
              " (('shutters', 'footman'), 1.0),\n",
              " (('sinning', 'sinned'), 1.0),\n",
              " (('skittishly', 'curvetting'), 1.0),\n",
              " (('slanderous', 'aspersion'), 1.0),\n",
              " (('slatternly', 'untidy'), 1.0),\n",
              " (('sleepiest', 'sunsets'), 1.0),\n",
              " (('slicings', 'severs'), 1.0),\n",
              " (('slipperiness', 'curb'), 1.0),\n",
              " (('slippering', 'misbehaviour'), 1.0),\n",
              " (('smackingly', 'feasted'), 1.0),\n",
              " (('smothering', 'conflagration'), 1.0),\n",
              " (('smugglers', 'contraband'), 1.0),\n",
              " (('smuggling', 'verbalists'), 1.0),\n",
              " (('snakes', 'sportively'), 1.0),\n",
              " (('soberly', 'recurred'), 1.0),\n",
              " (('softest', 'turkey'), 1.0),\n",
              " (('sog', 'sogger'), 1.0),\n",
              " (('soggy', 'squitchy'), 1.0),\n",
              " (('solaces', 'endearments'), 1.0),\n",
              " (('solander', 'cooke'), 1.0),\n",
              " (('sourceless', 'primogenitures'), 1.0),\n",
              " (('specialities', 'concentrations'), 1.0),\n",
              " (('spells', 'potencies'), 1.0),\n",
              " (('spicin', \"',--\"), 1.0),\n",
              " (('spiked', 'hotel'), 1.0),\n",
              " (('spiracles', 'apertures'), 1.0),\n",
              " (('spiritually', 'feasting'), 1.0),\n",
              " (('spleen', 'regulating'), 1.0),\n",
              " (('spontaneous', 'literal'), 1.0),\n",
              " (('sportively', 'festooning'), 1.0),\n",
              " (('spurrings', 'goadings'), 1.0),\n",
              " (('squadrons', 'asphaltites'), 1.0),\n",
              " (('stalwart', 'frames'), 1.0),\n",
              " (('statues', 'shields'), 1.0),\n",
              " (('stifle', 'upbraidings'), 1.0),\n",
              " (('stig', 'quig'), 1.0),\n",
              " (('stigma', 'originate'), 1.0),\n",
              " (('strenuous', 'exertions'), 1.0),\n",
              " (('stubble', 'laugheth'), 1.0),\n",
              " (('submission', 'endurance'), 1.0),\n",
              " (('submits', 'inspection'), 1.0),\n",
              " (('subordinately', 'emblazoned'), 1.0),\n",
              " (('subscribes', 'durand'), 1.0),\n",
              " (('substantive', 'deformity'), 1.0),\n",
              " (('subtly', 'malignantly'), 1.0),\n",
              " (('sufficit', 'rex'), 1.0),\n",
              " (('suicide', 'contributed'), 1.0),\n",
              " (('surly', 'dabs'), 1.0),\n",
              " (('surrenderest', 'hypo'), 1.0),\n",
              " (('swagger', 'unshunned'), 1.0),\n",
              " (('swayings', 'coyings'), 1.0),\n",
              " (('sweepers', 'billeted'), 1.0),\n",
              " (('sweetener', 'softener'), 1.0),\n",
              " (('sylla', 'classic'), 1.0),\n",
              " (('tee', 'twisk'), 1.0),\n",
              " (('tekel', 'upharsin'), 1.0),\n",
              " (('tenpin', 'andirons'), 1.0),\n",
              " (('terribly', 'infected'), 1.0),\n",
              " (('thief', 'catcher'), 1.0),\n",
              " (('thorkill', 'hake'), 1.0),\n",
              " (('throned', 'torsoes'), 1.0),\n",
              " (('thunderous', 'concussion'), 1.0),\n",
              " (('tiara', 'ewer'), 1.0),\n",
              " (('tic', 'dolly'), 1.0),\n",
              " (('tick', 'immaterial'), 1.0),\n",
              " (('ties', 'connexions'), 1.0),\n",
              " (('tinkerings', 'betters'), 1.0),\n",
              " (('tint', 'bestreaked'), 1.0),\n",
              " (('toils', 'trials'), 1.0),\n",
              " (('tongatobooarrs', 'erromanggoans'), 1.0),\n",
              " (('toothpick', 'rayther'), 1.0),\n",
              " (('torpid', 'intellects'), 1.0),\n",
              " (('torso', 'deceased'), 1.0),\n",
              " (('trained', 'chargers'), 1.0),\n",
              " (('transcendental', 'platonic'), 1.0),\n",
              " (('triangularly', 'platformed'), 1.0),\n",
              " (('trio', 'lancers'), 1.0),\n",
              " (('trover', 'litigated'), 1.0),\n",
              " (('tudors', 'bourbons'), 1.0),\n",
              " (('tutelary', 'guardian'), 1.0),\n",
              " (('twigs', 'productive'), 1.0),\n",
              " (('twiske', 'tee'), 1.0),\n",
              " (('tyre', 'carthage'), 1.0),\n",
              " (('ugliest', 'abortion'), 1.0),\n",
              " (('unachieved', 'revengeful'), 1.0),\n",
              " (('unassured', 'deprecating'), 1.0),\n",
              " (('unbiased', 'freewill'), 1.0),\n",
              " (('unbuckling', 'garters'), 1.0),\n",
              " (('uncanonical', 'rabbins'), 1.0),\n",
              " (('unceremoniously', 'bagged'), 1.0),\n",
              " (('unconditionally', 'reiterating'), 1.0),\n",
              " (('uncontaminated', 'aroma'), 1.0),\n",
              " (('undetected', 'villain'), 1.0),\n",
              " (('unequal', 'crosslights'), 1.0),\n",
              " (('unflattering', 'laureate'), 1.0),\n",
              " (('unhesitatingly', 'expert'), 1.0),\n",
              " (('unicorns', 'infesting'), 1.0),\n",
              " (('unintegral', 'mastery'), 1.0),\n",
              " (('unlock', 'bridegroom'), 1.0),\n",
              " (('unmanageably', 'winces'), 1.0),\n",
              " (('unmisgiving', 'hardihood'), 1.0),\n",
              " (('unmurmuringly', 'acquiesced'), 1.0),\n",
              " (('unnamable', 'imminglings'), 1.0),\n",
              " (('unpoetical', 'disreputable'), 1.0),\n",
              " (('unrecking', 'unworshipping'), 1.0),\n",
              " (('unsignifying', 'pettiness'), 1.0),\n",
              " (('unsocial', 'repelling'), 1.0),\n",
              " (('unstirring', 'paralysis'), 1.0),\n",
              " (('unsuccessful', 'onsets'), 1.0),\n",
              " (('unsurrenderable', 'wilfulness'), 1.0),\n",
              " (('untraceable', 'evolutions'), 1.0),\n",
              " (('untraditionally', 'independently'), 1.0),\n",
              " (('untrodden', 'unwilted'), 1.0),\n",
              " (('unwelcome', 'truths'), 1.0),\n",
              " (('vagueness', 'painfulness'), 1.0),\n",
              " (('vero', 'sufficit'), 1.0),\n",
              " (('versailles', 'beholder'), 1.0),\n",
              " (('vesper', 'hymns'), 1.0),\n",
              " (('vignettes', 'embellishments'), 1.0),\n",
              " (('villainies', 'detected'), 1.0),\n",
              " (('vineyards', 'champagne'), 1.0),\n",
              " (('vintage', 'vineyards'), 1.0),\n",
              " (('viol', 'spiracles'), 1.0),\n",
              " (('violate', 'pythagorean'), 1.0),\n",
              " (('virtuous', 'elder'), 1.0),\n",
              " (('vitus', 'imp'), 1.0),\n",
              " (('vividness', 'scorches'), 1.0),\n",
              " (('volatile', 'salts'), 1.0),\n",
              " (('voluntary', 'confiding'), 1.0),\n",
              " (('waded', 'nets'), 1.0),\n",
              " (('wainscots', 'reminding'), 1.0),\n",
              " (('waive', 'ceremonial'), 1.0),\n",
              " (('walfish', 'swedes'), 1.0),\n",
              " (('walter', 'canny'), 1.0),\n",
              " (('wandered', 'eddied'), 1.0),\n",
              " (('warbled', 'praises'), 1.0),\n",
              " (('warbling', 'persuasiveness'), 1.0),\n",
              " (('warmer', 'borneo'), 1.0),\n",
              " (('warringly', 'interweave'), 1.0),\n",
              " (('watcher', 'dietetically'), 1.0),\n",
              " (('wealthiest', 'praetorians'), 1.0),\n",
              " (('wery', 'woracious'), 1.0),\n",
              " (('wester', 'bombazine'), 1.0),\n",
              " (('westers', 'harmattans'), 1.0),\n",
              " (('whelmings', 'intermixingly'), 1.0),\n",
              " (('whisperingly', 'urging'), 1.0),\n",
              " (('whooping', 'imps'), 1.0),\n",
              " (('willis', 'ellery'), 1.0),\n",
              " (('wills', 'testaments'), 1.0),\n",
              " (('winces', 'unconcluded'), 1.0),\n",
              " (('wines', 'rhenish'), 1.0),\n",
              " (('wiping', 'profuse'), 1.0),\n",
              " (('witch', 'copenhagen'), 1.0),\n",
              " (('wonst', 'cibil'), 1.0),\n",
              " (('woo', 'hoo'), 1.0),\n",
              " (('worldly', 'ties'), 1.0),\n",
              " (('worming', 'undulation'), 1.0),\n",
              " (('wounds', 'bleed'), 1.0),\n",
              " (('wrangling', 'scuffling'), 1.0),\n",
              " (('wrapall', 'dreadnaught'), 1.0),\n",
              " (('zig', 'zag'), 1.0),\n",
              " (('fluke', 'chains'), 0.9920634920634921),\n",
              " (('right', 'whale'), 0.9841762792073351),\n",
              " (('martha', 'vineyard'), 0.9642857142857143),\n",
              " (('steering', 'oar'), 0.9423076923076923),\n",
              " (('hither', 'thither'), 0.8928571428571429),\n",
              " (('epaulets', 'epaulets'), 0.8888888888888888),\n",
              " (('inclined', 'plane'), 0.8888888888888888),\n",
              " (('marling', 'spike'), 0.8888888888888888),\n",
              " (('massachusetts', 'calendar'), 0.8888888888888888),\n",
              " (('riding', 'whips'), 0.8888888888888888),\n",
              " (('smoothe', 'seam'), 0.8888888888888888),\n",
              " (('thou', 'art'), 0.8827566485559051),\n",
              " (('loose', 'fish'), 0.8507359307359308),\n",
              " (('sneezes', ')--'), 0.84375),\n",
              " (('huzza', 'porpoise'), 0.8223684210526315),\n",
              " (('centuries', 'ago'), 0.8166666666666667),\n",
              " (('admirable', 'artistic'), 0.8),\n",
              " (('beef', 'bread'), 0.8),\n",
              " (('cousin', 'hosea'), 0.8),\n",
              " (('cows', 'calves'), 0.8),\n",
              " (('dan', 'coopman'), 0.8),\n",
              " (('furthest', 'bounds'), 0.8),\n",
              " (('leathern', 'tally'), 0.8),\n",
              " (('reverend', 'clergy'), 0.8),\n",
              " (('tier', 'butts'), 0.8),\n",
              " (('king', 'post'), 0.7974481658692185),\n",
              " (('good', 'bye'), 0.7824074074074074),\n",
              " (('fiery', 'pit'), 0.7440476190476191),\n",
              " (('rose', 'bud'), 0.7142857142857143),\n",
              " (('kings', 'queens'), 0.7111111111111111),\n",
              " (('look', 'outs'), 0.7093596059113301),\n",
              " (('cutting', 'spade'), 0.6918918918918919),\n",
              " (('horse', 'shoe'), 0.6868131868131868),\n",
              " (('ivory', 'leg'), 0.6771669341894061),\n",
              " (('arrayed', 'decent'), 0.6666666666666666),\n",
              " (('beale', 'bennett'), 0.6666666666666666),\n",
              " (('brazil', 'banks'), 0.6666666666666666),\n",
              " (('copy', 'blackstone'), 0.6666666666666666),\n",
              " (('fore', 'aft'), 0.6666666666666666),\n",
              " (('ply', 'shuttle'), 0.6666666666666666),\n",
              " (('reef', 'topsails'), 0.6666666666666666),\n",
              " (('speechless', 'placeless'), 0.6666666666666666),\n",
              " (('wrists', 'ankles'), 0.6666666666666666),\n",
              " (('never', 'mind'), 0.6475703946968315),\n",
              " (('battering', 'ram'), 0.6428571428571429),\n",
              " (('congregational', 'church'), 0.6428571428571429),\n",
              " (('sumatra', 'java'), 0.6428571428571429),\n",
              " (('latitudes', 'longitudes'), 0.6),\n",
              " (('life', 'buoy'), 0.598740440845704),\n",
              " (('caulk', 'seams'), 0.5714285714285714),\n",
              " (('dilated', 'nostrils'), 0.5714285714285714),\n",
              " (('gentle', 'globules'), 0.5714285714285714),\n",
              " (('grotesque', 'figures'), 0.5714285714285714),\n",
              " (('kindled', 'shavings'), 0.5714285714285714),\n",
              " (('pine', 'tree'), 0.5681818181818182),\n",
              " (('actively', 'engaged'), 0.5625),\n",
              " (('full', 'grown'), 0.5454918032786885),\n",
              " (('gay', 'header'), 0.5333333333333333),\n",
              " (('moth', 'rust'), 0.5333333333333333),\n",
              " (('plum', 'pudding'), 0.5333333333333333),\n",
              " (('robust', 'healthy'), 0.5333333333333333),\n",
              " (('tablet', 'erected'), 0.5333333333333333),\n",
              " (('thrasher', 'killer'), 0.5333333333333333),\n",
              " (('bashee', 'isles'), 0.5294117647058824),\n",
              " (('new', 'zealand'), 0.5051546391752577),\n",
              " (('16th', '1851'), 0.5),\n",
              " (('1779', 'disinterred'), 0.5),\n",
              " (('1807', 'totally'), 0.5),\n",
              " (('1839', 'unfitness'), 0.5),\n",
              " (('21st', 'june'), 0.5),\n",
              " (('31st', '1839'), 0.5),\n",
              " (('abed', 'unendurable'), 0.5),\n",
              " (('abjectly', 'reduced'), 0.5),\n",
              " (('abomination', 'brook'), 0.5),\n",
              " (('abundantly', 'picturesquely'), 0.5),\n",
              " (('accelerating', 'momentary'), 0.5),\n",
              " (('accumulate', 'princely'), 0.5),\n",
              " (('accumulating', 'piling'), 0.5),\n",
              " (('accurately', 'foretell'), 0.5),\n",
              " (('adjacent', 'interdicted'), 0.5),\n",
              " (('admirably', 'satirical'), 0.5),\n",
              " (('affords', 'fewer'), 0.5),\n",
              " (('afire', 'drenching'), 0.5),\n",
              " (('agile', 'obstetrics'), 0.5),\n",
              " (('aglow', 'bridegrooms'), 0.5),\n",
              " (('agonized', 'respirations'), 0.5),\n",
              " (('aldermen', 'patent'), 0.5),\n",
              " (('allowance', 'exclusive'), 0.5),\n",
              " (('alpacas', 'volcanoes'), 0.5),\n",
              " (('alternate', 'fitful'), 0.5),\n",
              " (('amplified', 'fortifications'), 0.5),\n",
              " (('amusing', 'pluck'), 0.5),\n",
              " (('ancientest', 'draughts'), 0.5),\n",
              " (('angelo', 'paints'), 0.5),\n",
              " (('animation', 'unexpected'), 0.5),\n",
              " (('ankers', 'geneva'), 0.5),\n",
              " (('anne', 'forthing'), 0.5),\n",
              " (('anoint', 'machinery'), 0.5),\n",
              " (('antiquities', 'throned'), 0.5),\n",
              " (('apostolic', 'lancer'), 0.5),\n",
              " (('appallingly', 'astonishing'), 0.5),\n",
              " (('appearing', 'stiletto'), 0.5),\n",
              " (('appearing', 'yawing'), 0.5),\n",
              " (('arbitrary', 'vein'), 0.5),\n",
              " (('archaeological', 'fossiliferous'), 0.5),\n",
              " (('archbishop', 'savesoul'), 0.5),\n",
              " (('archer', 'amusing'), 0.5),\n",
              " (('arching', 'segment'), 0.5),\n",
              " (('architect', 'builder'), 0.5),\n",
              " (('artedi', 'sibbald'), 0.5),\n",
              " (('artificially', 'upheld'), 0.5),\n",
              " (('aspirations', 'prematurely'), 0.5),\n",
              " (('aspiring', 'rainbowed'), 0.5),\n",
              " (('assail', 'fatally'), 0.5),\n",
              " (('assailants', 'divinity'), 0.5),\n",
              " (('assailed', 'yells'), 0.5),\n",
              " (('associates', 'stung'), 0.5),\n",
              " (('atheism', 'shrink'), 0.5),\n",
              " (('atlantics', 'pacifics'), 0.5),\n",
              " (('attainable', 'felicity'), 0.5),\n",
              " (('attendant', 'desiring'), 0.5),\n",
              " (('attest', 'cetus'), 0.5),\n",
              " (('attuned', 'gradual'), 0.5),\n",
              " (('authentic', 'abortions'), 0.5),\n",
              " (('autumn', 'zoned'), 0.5),\n",
              " (('available', 'systematizer'), 0.5),\n",
              " (('awls', 'pens'), 0.5),\n",
              " (('axles', 'carriages'), 0.5),\n",
              " (('babbling', 'betake'), 0.5),\n",
              " (('baffled', 'channel'), 0.5),\n",
              " (('bales', 'jars'), 0.5),\n",
              " (('ballasted', 'utilities'), 0.5),\n",
              " (('bally', 'timor'), 0.5),\n",
              " (('baptizo', 'nomine'), 0.5),\n",
              " (('baronial', 'vassal'), 0.5),\n",
              " (('bashaw', 'assails'), 0.5),\n",
              " (('bathing', 'bath'), 0.5),\n",
              " (('beads', 'ornaments'), 0.5),\n",
              " (('becket', 'bled'), 0.5),\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUJTsI4oT2aQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba60733b-f063-4fe2-f0ce-1d5b56850f68"
      },
      "source": [
        "Counter(collocations).most_common(20)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('moby_dick', 83.0),\n",
              " ('sperm_whale', 20.002847184002935),\n",
              " ('mrs_hussey', 10.5625),\n",
              " ('mast_heads', 4.391152941176471),\n",
              " ('sag_harbor', 4.0),\n",
              " ('vinegar_cruet', 4.0),\n",
              " ('try_works', 3.7944046844502277),\n",
              " ('dough_boy', 3.7067873303167422),\n",
              " ('white_whale', 3.698807453416149),\n",
              " ('caw_caw', 3.4722222222222223),\n",
              " ('samuel_enderby', 3.4285714285714284),\n",
              " ('cape_horn', 3.4133333333333336),\n",
              " ('new_bedford', 3.3402061855670104),\n",
              " ('quarter_deck', 3.2339339991315676),\n",
              " ('deacon_deuteronomy', 3.2),\n",
              " ('father_mapple', 3.0),\n",
              " ('gamy_jesty', 3.0),\n",
              " ('hoky_poky', 3.0),\n",
              " ('jesty_joky', 3.0),\n",
              " ('joky_hoky', 3.0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "478Sf80ZT2aS"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "Extract the top 10 collocations for the Twitter data. You need to preprocess the data first!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUPSMotsT2aS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b9322c9-e115-437b-981a-9a8031404fff"
      },
      "source": [
        "stopwords_ = set(stopwords.words('english'))\r\n",
        "\r\n",
        "tweets = [line.strip() for line in open('tweets_en.txt', encoding='utf8')]\r\n",
        "\r\n",
        "def preprocessing(tweet):\r\n",
        "  tweet = ' '.join([w.lower() for w in tweet.split() if w not in stopwords_])\r\n",
        "  #remove stopwords, remove links, remove emojis , remove mentions, remove hashtags , remove numbers\r\n",
        "  url_pattern = re.compile('https?://[A-Za-z0-9\\.-_]*/[A-Za-z0-9\\.-_]*') \r\n",
        "  tweet = re.sub(url_pattern, '', tweet)\r\n",
        "  user_names_pattern = re.compile('@[A-Za-z0-9\\.-_]+')\r\n",
        "  tweet = re.sub(user_names_pattern, '', tweet)\r\n",
        "  hashtag_pattern = re.compile('#[\\w]*')\r\n",
        "  tweet = re.sub(hashtag_pattern, '', tweet)\r\n",
        "  emojis_pattern = re.compile(pattern = \"[\"\r\n",
        "          u\"\\U0001F600-\\U0001F64F\"  # emoticons\r\n",
        "          u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\r\n",
        "          u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\r\n",
        "          u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\r\n",
        "          u\"\\u2600-\\u26FF\\u2700-\\u27BF\"\r\n",
        "                            \"]+\", flags = re.UNICODE)\r\n",
        "  tweet = re.sub(emojis_pattern, '', tweet)\r\n",
        "\r\n",
        "  numbers_patterns = re.compile('[0-9]+[\\w]*')\r\n",
        "  tweet = re.sub(numbers_patterns, '', tweet)\r\n",
        "\r\n",
        "  punctiation_pattern = re.compile('[!-_@#$%^&*()?<>;\\.,:\"]')\r\n",
        "  tweet = re.sub(punctiation_pattern, '', tweet) \r\n",
        "\r\n",
        "  return tweet\r\n",
        "\r\n",
        "cleaned_tweets = [preprocessing(tweet) for tweet in tweets]\r\n",
        "print(tweets[500:520])\r\n",
        "print(cleaned_tweets[500:520])\r\n",
        "\r\n",
        "words = [word.lower() for tweet in cleaned_tweets for word in tweet.split() \r\n",
        "         if len(word) > 2 ]\r\n",
        "         \r\n",
        "finder = BigramCollocationFinder.from_words(words)\r\n",
        "bgm = BigramAssocMeasures()\r\n",
        "score = bgm.mi_like\r\n",
        "collocations = {'_'.join(bigram): pmi for bigram, pmi in finder.score_ngrams(score)}\r\n",
        "#collocations\r\n",
        "\r\n",
        "Counter(collocations).most_common(20)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"I'm at Warrington Central Railway Station (WAC) (Warrington, Cheshire) http://t.co/UNOZyxhVg6\", 'I looked liked such a wanker in red shorts today when we were playing in our blue strip #Schoolboyerror', '@SUNSHHHEEEIINNE deaths = ratings. Everyone must have a mick foley attitude', 'About to watch #raw #spoiler alert @LeviKitson http://t.co/PmFuOIBOjR', 'I love justin', 'Thumbs up as, I tweet, from my 8yr old. We eat @ different times on a tues. Cooking it this way means no soggy pastry http://t.co/jtRj2ONA1X', \"Don't say I'm better off dead, 'cause heavens full and hell doesn't want me.\", 'Wtf my insurance is still gonna be under ¬£700 even with ¬£100 excess haha #winning', '@JacobMcparland where you watching football at?', \"People that spell school 'skool' should go back there.\", \"@mrpeterandre I've never been more devastated in my whole life, won tickets to Gilgamesh and you aren't going to be there #hatemylife\", 'Think about what to think.', \"Can't wait to start going on the sunbeds again üòÅ need to be brown\", '@AlexCStanley you could always delete the Tweet http://t.co/vv9K4qiPSY', \"@JosieCook_ yeah buts it's in my car at home... #0help\", 'Getting smashed in Birmingham', '@emilykatebrown_ @erinfleminggg @lydiagracedunn @ktmonk_ no it is isnt it cause I said to el franko do we each get a photo and he said yeahh', '@nicwilkins85 it was a couple a freakin hours lol x', 'Forever dreaming of a winning bet that means I buy some trainers and still have change!', \"Sick of the gym it's really no doing anything for me üòî\"]\n",
            "['im warrington central railway station wac warrington cheshire ', 'i looked liked wanker red shorts today playing blue strip ', ' deaths  ratings everyone must mick foley attitude', 'about watch   alert  ', 'i love justin', 'thumbs as i tweet  old we eat  different times tues cooking way means soggy pastry ', 'dont say im better dead cause heavens full hell want me', 'wtf insurance still gonna ¬£ even ¬£ excess haha ', ' watching football at', 'people spell school skool go back there', ' ive never devastated whole life tickets gilgamesh going ', 'think think', 'cant wait start going sunbeds  need brown', ' could always delete tweet ', ' yeah buts car home ', 'getting smashed birmingham', '    isnt cause i said el franko get photo said yeahh', ' couple freakin hours lol x', 'forever dreaming winning bet means i buy trainers still change', 'sick gym really anything ']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('temperature_rain', 183.11751137578898),\n",
              " ('wind_mph', 155.16358564189488),\n",
              " ('cant_wait', 105.13793208133924),\n",
              " ('last_night', 79.39003525812679),\n",
              " ('slowly_temperature', 75.07782138676582),\n",
              " ('looking_forward', 69.01986497537506),\n",
              " ('mph_barometer', 51.48125544899739),\n",
              " ('happy_birthday', 49.91443818065343),\n",
              " ('barometer_hpa', 35.55359336609337),\n",
              " ('today_humidity', 34.377616000084835),\n",
              " ('rain_today', 26.259736082602583),\n",
              " ('falling_slowly', 21.991356382978722),\n",
              " ('railway_station', 20.288212986610944),\n",
              " ('arctic_monkeys', 20.031772575250837),\n",
              " ('rising_slowly', 18.086537050623626),\n",
              " ('cant_believe', 17.656357621879035),\n",
              " ('fingers_crossed', 17.527572016460905),\n",
              " ('jeremy_kyle', 16.329142488716958),\n",
              " ('advent_calendar', 15.419772256728779),\n",
              " ('paul_walker', 12.849506578947368)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JA4YTQwXpllu",
        "outputId": "060051b4-c369-4cdb-fc90-cd5a42b493d0"
      },
      "source": [
        "#Or other solution by using emoji module\r\n",
        "#https://pypi.org/project/emoji/\r\n",
        "! pip install emoji\r\n",
        "import emoji\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikA0WRqThSUb",
        "outputId": "3790c00c-e96f-4f63-cdc0-10e3091f4110"
      },
      "source": [
        "import spacy \r\n",
        "import re\r\n",
        "from nltk.corpus import stopwords\r\n",
        "\r\n",
        "nlp = spacy.load('en')\r\n",
        "stop_words = [w.lower() for w in stopwords.words()]\r\n",
        "#for textual emoji\r\n",
        "emoticon_string = r\"\"\"\r\n",
        "(?:\r\n",
        "[<>]?\r\n",
        "[:;=8] #eyes\r\n",
        "[\\-o\\*\\']? #optional nose\r\n",
        "[\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] #mouth\r\n",
        "|\r\n",
        "[\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] #mouth\r\n",
        "[\\-o\\*\\']? #optional nose\r\n",
        "[:;=8] #eyes\r\n",
        "[<>]?\r\n",
        ")\"\"\"\r\n",
        "\r\n",
        "#remove graphical emoji\r\n",
        "def give_emoji_free_text(text):\r\n",
        "  return emoji.get_emoji_regexp().sub(r'',text)\r\n",
        "\r\n",
        "def sanitize(string):\r\n",
        "  \"\"\" Sanitize one string \"\"\"\r\n",
        "\r\n",
        "  #remove graphical emoji\r\n",
        "  string = give_emoji_free_text(string)\r\n",
        "\r\n",
        "  #remove textual emoji\r\n",
        "  string = re.sub(emoticon_string, '', string)\r\n",
        "\r\n",
        "  #normalize to lowercase\r\n",
        "  string = string.lower()\r\n",
        "\r\n",
        "  # spacy tokenizer\r\n",
        "  string_split = [token.text for token in nlp(string)]\r\n",
        "\r\n",
        "  #in case the string is empty \r\n",
        "  if not string_split:\r\n",
        "    return ''\r\n",
        "\r\n",
        "  #remove user, assuming user has @ in front\r\n",
        "  names = re.compile('@[A-Za-z0-9_][A-Za-z0-9_]+')\r\n",
        "  string = re.sub(names, '', string)\r\n",
        "\r\n",
        "  #remove 't.co/' links\r\n",
        "  string = re.sub(r'http://t.co\\/[^\\s]+', '', string, flags=re.MULTILINE)\r\n",
        "\r\n",
        "  #remove # symbol and punctuations\r\n",
        "  for punc in '\"?,/.:!#()\"\"':\r\n",
        "    string = string.replace(punc, '')\r\n",
        "\r\n",
        "  #removing stopwords \r\n",
        "  string = ' '.join([w for w in string.split() if w not in stop_words])\r\n",
        "  return string\r\n",
        "\r\n",
        "list_sanitized = [sanitize(string) for string in tweets[:300]]\r\n",
        "[(tweets[i], list_sanitized[i]) for i in range(0,40)]"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('@cosmetic_candy I think a lot of people just enjoy being a pain in the ass on there',\n",
              "  'think lot people enjoy pain ass'),\n",
              " ('Best get ready sunbed and dinner with nana today :)',\n",
              "  'best get ready sunbed dinner nana today'),\n",
              " ('@hardlyin70 thats awesome!', 'thats awesome'),\n",
              " ('Loving this weather', 'loving weather'),\n",
              " ('‚Äú@danny_boy_37: Just seen an absolute idiot in shorts! Be serious!‚Äù Desperado gentleman',\n",
              "  '‚Äú seen absolute idiot shorts serious‚Äù desperado gentleman'),\n",
              " ('@SamanthaOrmerod trying to resist a hardcore rave haha! Resisting towns a doddle! Posh dance floor should wear them in quite easy xx',\n",
              "  'trying resist hardcore rave haha resisting towns doddle posh dance floor wear quite easy xx'),\n",
              " ('59 days until @Beyonce!!! Wooo @jfracassini #cannotwait',\n",
              "  '59 days wooo cannotwait'),\n",
              " ('That was the dumbest tweet i ever seen', 'dumbest tweet ever seen'),\n",
              " ('Oh what to do on this fine sunny day?', 'oh fine sunny day'),\n",
              " ('@Brooke_C_X hows the fish ? Hope they r ok. Xx', 'hows fish hope r ok xx'),\n",
              " ('@Jbowe_ üò†', ''),\n",
              " ('Or this @louise_munchi http://t.co/Gsb7V1oVLU', ''),\n",
              " ('@guy_clifton your diary is undoubtedly busier than mine, but feel free to check http://t.co/0pjNL1uwU9',\n",
              "  'diary undoubtedly busier feel free check'),\n",
              " ('Willy', 'willy'),\n",
              " ('@StephanieLee__ congrats gorgeous xxx', 'congrats gorgeous xxx'),\n",
              " ('Puppies are hard work!!! So rewarding though, I love my little Bovril so much! http://t.co/a8cHDbGUKo',\n",
              "  'puppies hard work rewarding though love little bovril much'),\n",
              " ('Hungover banter lol hehe what http://t.co/eqbAFlyHng',\n",
              "  'hungover banter lol hehe'),\n",
              " ('@peterbaird5 Then come down to London and see him. Lol :-) xxxx',\n",
              "  'london see lol - xxxx'),\n",
              " (\"i'm not going to wear makeup today #whocares #nobody\",\n",
              "  \"i'm going wear makeup today whocares nobody\"),\n",
              " ('@GreigSweeney whit', 'whit'),\n",
              " ('Got ready well too early #keenlad', 'got ready well early keenlad'),\n",
              " (\"@LukeyR9 @marcyearling17 @dcah_4 @syearling @ricknose91 we need to start losing so we don't get promoted\",\n",
              "  'need start losing get promoted'),\n",
              " (\"It's like being In a different country, this weather!\",\n",
              "  'like different country weather'),\n",
              " ('@michalgarnett @scottdavies90 why lie?', 'lie'),\n",
              " (\"I think it's safe to say my head now hurts #oww @ Mahiki http://t.co/KTLJ97Hia5\",\n",
              "  'think safe say head hurts oww @ mahiki'),\n",
              " ('Argh, i would love a tattoo here http://t.co/RGwjpA699T',\n",
              "  'argh would love tattoo'),\n",
              " ('@hannahkerr3 hahahahaha', 'hahahahaha'),\n",
              " ('@ScarletSophie you seriously do ^.^ miss you chickadee xxxx',\n",
              "  'seriously ^^ miss chickadee xxxx'),\n",
              " ('@SW16Massage sounds good, I want some of that too...', 'sounds good'),\n",
              " (\"Millie's cookies for breakfastüòä\", \"millie's cookies breakfast\"),\n",
              " ('the head office is my head', 'head office head'),\n",
              " ('Listening to #kisstory all the way from Madeira and its sounding amazing in the sunshine #kisstoryinthesun http://t.co/MkkvxGmwKI',\n",
              "  'listening kisstory way madeira sounding amazing sunshine kisstoryinthesun'),\n",
              " ('Yay! Heated discussions about education in the office. There are few things people get quite so passionate about as treatment of children.',\n",
              "  'yay heated discussions education office things people get quite passionate treatment children'),\n",
              " ('Day is dragging', 'day dragging'),\n",
              " ('@JBieberBubba thats late. Asshole. Seriously. Who does he think he is!',\n",
              "  'thats late asshole seriously think'),\n",
              " (\"Next weekend. I WILL go to the cinema &amp; watch my future husband (Channing) in 'Side Effects'\",\n",
              "  \"next weekend go cinema &amp; watch future husband channing 'side effects'\"),\n",
              " (\"@Bulliopr oh Paul, it's not even lunch time yet!\",\n",
              "  'oh paul even lunch time yet'),\n",
              " ('@correr46 this is how I like doing laundry :-) http://t.co/xK1YoHQMgd',\n",
              "  'like laundry -'),\n",
              " ('@nataliemunro19 @neeen95 here yellow hair is da boy', 'yellow hair boy'),\n",
              " ('More morning emails with movie producers #anotherdayanotherdoller #dowork',\n",
              "  'morning emails movie producers anotherdayanotherdoller dowork')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VKNM_b5hSZc",
        "outputId": "98c2b3c7-306f-4e8f-a8b3-8fb592947916"
      },
      "source": [
        "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from collections import Counter\r\n",
        "\r\n",
        "words = [word for document in list_sanitized for word in document.split() if len(word) > 2]\r\n",
        "\r\n",
        "finder = BigramCollocationFinder.from_words(words)\r\n",
        "bgm = BigramAssocMeasures()\r\n",
        "score = bgm.mi_like\r\n",
        "collocations = {'_'.join(bigram): pmi for bigram, pmi in finder.score_ngrams(score)}\r\n",
        "#collocations\r\n",
        "Counter(collocations).most_common(20)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('fingers_crossed', 2.0),\n",
              " (\"''did_tattoos\", 1.0),\n",
              " (\"''you're_tall''\", 1.0),\n",
              " (\"'blow_go'\", 1.0),\n",
              " (\"'out_order'\", 1.0),\n",
              " (\"'piss_off'\", 1.0),\n",
              " (\"'side_effects'\", 1.0),\n",
              " (\"'when_dolmio\", 1.0),\n",
              " (\"'you_wot\", 1.0),\n",
              " ('00mm_hum', 1.0),\n",
              " ('100231mb_temp', 1.0),\n",
              " ('13th_thurs', 1.0),\n",
              " ('14th_fri', 1.0),\n",
              " ('150mb_data', 1.0),\n",
              " ('153_miles', 1.0),\n",
              " ('25mph_baro', 1.0),\n",
              " (\"2pprizesplease_how's\", 1.0),\n",
              " ('5thhashtag_cmyk', 1.0),\n",
              " ('60d_lenses', 1.0),\n",
              " (\"6ft_''did\", 1.0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    }
  ]
}