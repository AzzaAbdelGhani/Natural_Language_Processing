{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"Information Retrieval.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"z3QKP_UCT2ZY"},"source":["## Regular Expressions\n","\n","Defining REs in Python is straightforward:"]},{"cell_type":"code","metadata":{"id":"fvtuSQ_KT2Za"},"source":["import re\n","\n","pattern = re.compile('[bcrh]at')\n","pattern2 = re.compile('(.*)([bcrh]at)(.*)')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6aUlppMLT2Ze"},"source":["We can then use the pattern to `search()` or `match()` strings to it. \n","\n","`search()` will return a result if the pattern occurs **anywhere** in the input string.\n","\n","`match()` will only return a result if the pattern **completely** matches the input string."]},{"cell_type":"code","metadata":{"id":"E-SDdrJQT2Zf"},"source":["word = 'the batter won the game'\n","matches = re.match(pattern2, word) # won't return a a result, i.e., matches = None\n","searches = re.search(pattern, word) # finds a substring"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WA0p84TUT2Zh"},"source":["print(matches.groups())\n","print(searches)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eSrth_iAT2Zl"},"source":["Both have a number of attributes to access the results. \n","- `span()` gives us a tuple of the substring that matches\n","- `group()`returns the matched substring"]},{"cell_type":"code","metadata":{"id":"Ddb7SJqoT2Zl"},"source":["span = searches.span()\n","word[span[0]:span[1]], span"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rn3u2KVJT2Zn"},"source":["searches.group()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r1U0OciCT2Zp"},"source":["If we have used several RE groups (in brackets `()`), we can access them individually via `groups()`"]},{"cell_type":"code","metadata":{"id":"rqav7Tb9T2Zt"},"source":["word = 'preconstitutionalism'\n","affixes = re.compile('(...).+(...)')\n","re.search(affixes, word).groups()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_Ifp6DaKT2Zw"},"source":["For the email address finder, we can use a more advanced pattern and test it:"]},{"cell_type":"code","metadata":{"id":"xlECD_hNT2Zw"},"source":["email = re.compile('^[A-Za-z0-9][A-Za-z0-9\\.-]*@[A-Za-z0-9][A-Za-z0-9\\.-]+\\.[A-Za-z0-9\\.-][A-Za-z0-9\\.-][A-Za-z0-9\\.-]?$')\n","# for address in ['me.@unibocconi.it', '@web.de', '.@gmx.com', 'not working@aol.com']:\n","\n","for address in 'notMyFault@webmail.com,smithie123@gmx,Free stuff@unibocconi.it,mark_my_words@hotmail;com,truthOrDare@webmail.in,look@me@twitter.com,how2GetAnts@aol.dfdsfgfdsgfd'.split(','):\n","    print(address, re.match(email, address))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oDGTQciGT2Zy"},"source":["We can also use the pattern to replace elements of a string that match with `sub()`"]},{"cell_type":"code","metadata":{"id":"cLhT-KCDT2Zz"},"source":["print('Are you all awake?'.replace('???', '!'))\n","\n","numbers = re.compile('[0-9]')\n","re.sub(numbers, '0', 'Back in the 90s, when I was a 12-year-old, a CD cost just 15,99EUR!')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"70OwFvZlT2Z2"},"source":["## Exercise\n","\n","Write a RegEx to remove all user names from the tweets and replace them with the token \"@USER\""]},{"cell_type":"code","metadata":{"id":"P9bJ0_itWjnW"},"source":["! pip install wget"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TIdcBkkDWjIV"},"source":["import wget\n","url = 'https://raw.githubusercontent.com/dirkhovy/NLPclass/master/data/tweets_en.txt'\n","wget.download(url, 'tweets_en.txt')\n","tweets = [line.strip() for line in open('tweets_en.txt', encoding='utf8')]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0V8cH3zcT2Z2"},"source":["# your code here\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hueOZuI9WqNk"},"source":["Now, write a RegEx to extract all user names from the tweets\n"]},{"cell_type":"code","metadata":{"id":"njHhaKfcWxbb"},"source":["# your code here\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"txOet6tET2Z6"},"source":["## Exercise\n","\n","Write a RegEx to search for all hashtags containing the word `good` in them."]},{"cell_type":"code","metadata":{"id":"GbECIdiXT2Z6"},"source":["# your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1TvaZX2iT2Z8"},"source":["## TF-IDF\n","\n","Let's extract the most important words from Moby Dick"]},{"cell_type":"code","metadata":{"id":"ugMRmSObW6SS"},"source":["url = 'https://raw.githubusercontent.com/dirkhovy/NLPclass/master/data/moby_dick.txt'\n","wget.download(url, 'moby_dick.txt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tP9bo4MlT2Z8"},"source":["import pandas as pd\n","documents = [line.strip() for line in open('moby_dick.txt', encoding='utf8')]\n","print(documents[1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kF52UbzbT2Z-"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","tfidf_vectorizer = TfidfVectorizer(analyzer='word',\n","                                   min_df=0.001,\n","                                   max_df=0.75,\n","                                   stop_words='english',\n","                                   sublinear_tf=True)\n","\n","X = tfidf_vectorizer.fit_transform(documents)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F3BcEHFMT2Z_"},"source":["X.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NlzSMGTyT2aB"},"source":["Now, let's get the same information as raw counts:"]},{"cell_type":"code","metadata":{"id":"j-eYAlQoT2aB"},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","vectorizer = CountVectorizer(analyzer='word', min_df=0.001, max_df=0.75, stop_words='english')\n","\n","X2 = vectorizer.fit_transform(documents)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ndYI5uoGT2aD"},"source":["X.shape, X2.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WTa0ukM-T2aG"},"source":["df = pd.DataFrame(data={'word': vectorizer.get_feature_names(), \n","                        'tf': X2.sum(axis=0).A1, \n","                        'idf': tfidf_vectorizer.idf_,\n","                        'tfidf': X.sum(axis=0).A1\n","                       })"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tPdLl1HlewVd"},"source":["X2.sum(axis=0).A1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Lb7flVdT2aI"},"source":["df = df.sort_values(['tfidf', 'tf', 'idf'], ascending=False)\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VvOWdD-CfCwS"},"source":["df = df.sort_values(['tf', 'idf'], ascending=False)\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r1JwqX8nT2aK"},"source":["## Exercise\n","Extract **only** the bigrams (no unigrams) from Moby Dick and find the top 10 in terms of TF-IDF."]},{"cell_type":"code","metadata":{"id":"UYjTXRXJT2aK"},"source":["# your code \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xz1deDO7T2aN"},"source":["## PMI\n","Extracting PMI from text is relatively straightforward, and `nltk` offer some functions to do so flexibly."]},{"cell_type":"code","metadata":{"id":"yDE5YBHZT2aN"},"source":["import nltk\n","nltk.download('all')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"xMeicqRyT2aP"},"source":["from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n","from nltk.corpus import stopwords\n","from collections import Counter\n","\n","stopwords_ = set(stopwords.words('english'))\n","\n","words = [word.lower() for document in documents for word in document.split() \n","         if len(word) > 2 \n","         and word not in stopwords_]\n","         \n","finder = BigramCollocationFinder.from_words(words)\n","bgm = BigramAssocMeasures()\n","score = bgm.mi_like\n","collocations = {'_'.join(bigram): pmi for bigram, pmi in finder.score_ngrams(score)}\n","collocations"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YWtNCk8ojYRd"},"source":["finder.score_ngrams(score)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nUJTsI4oT2aQ"},"source":["Counter(collocations).most_common(20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"478Sf80ZT2aS"},"source":["## Exercise\n","\n","Extract the top 10 collocations for the Twitter data. You need to preprocess the data first!"]},{"cell_type":"code","metadata":{"id":"yUPSMotsT2aS"},"source":["# your code here"],"execution_count":null,"outputs":[]}]}